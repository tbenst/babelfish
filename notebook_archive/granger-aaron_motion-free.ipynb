{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster.bicluster import SpectralBiclustering\n",
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "from sklearn.metrics import consensus_score\n",
    "\n",
    "import torch as T\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import scipy.linalg\n",
    "from functools import partial\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sparse\n",
    "from scipy import stats\n",
    "from __future__ import print_function\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from pandas import DataFrame\n",
    "\n",
    "import os, sys, datetime\n",
    "LF_CODE_PATH = os.path.expanduser('~/projects/LFAnalyze/code')\n",
    "FT_CODE_PATH = os.path.expanduser('~/projects/fishTrax/code/analysis/')\n",
    "FD_CODE_PATH = os.path.expanduser('~/projects/fish_despair_notebooks/src/')\n",
    "sys.path.insert(0,LF_CODE_PATH)\n",
    "sys.path.insert(0,FT_CODE_PATH)\n",
    "sys.path.insert(0,FD_CODE_PATH)\n",
    "\n",
    "import passivity_2p_imaging_utils as p2putils\n",
    "import visualization_utils as vizutil\n",
    "reload(p2putils)\n",
    "tmp_dir = '/tmp/'\n",
    "all_data = p2putils.get_all_datasets(tmp_dir=tmp_dir)\n",
    "\n",
    "f = all_data['e'][2]\n",
    "\n",
    "# df, sig = f.get_cnmf_roi_table_and_signals()\n",
    "M = f.get_signals_raw(z=None)\n",
    "\n",
    "# data = np.load(\"../cnmf_f01555.npz\")\n",
    "# cnmf = data['cnmf'].astype(np.float32)\n",
    "# raw = data['raw'].astype(np.float32)\n",
    "# del data\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    # a is a signal\n",
    "    ret = np.cumsum(a,0) # sum over time\n",
    "    ret[n:] = ret[n:] - ret[:-n] # diff of n samples back\n",
    "    rm = ret[n - 1:] / n\n",
    "    pad_start = np.full((n-1,rm.shape[1]), rm[0])\n",
    "    return np.vstack([pad_start, rm])\n",
    "\n",
    "def ewma(data,span):\n",
    "    \"exponential weighted moving average.\"\n",
    "    df = DataFrame(data)\n",
    "    return df.ewm(span).mean().values\n",
    "\n",
    "def df_f(x,ma_window=6,span=6):\n",
    "    u = moving_average(x,ma_window)\n",
    "    return ewma((x - u)/u, span)\n",
    "\n",
    "# neurons = sig.T\n",
    "neurons = M.T\n",
    "\n",
    "neuron_ids = np.sort(np.argsort(neurons.std(0))[-5000:])\n",
    "df = f.get_roi_table()\n",
    "df = df.iloc[neuron_ids]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "neurons = neurons[:,neuron_ids]\n",
    "\n",
    "neurons = T.from_numpy(df_f(neurons).astype(np.float32)).cuda()\n",
    "neurons = (neurons - neurons.mean(0))/(neurons.std(0)+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_before = 1\n",
    "exclude_after = 4\n",
    "motion_frames = numpy.searchsorted(f.frame_et[:,-1], f.tail_movement_start_times,side=\"left\")\n",
    "exclude_idx = np.unique((motion_frames[:,None] + np.arange(-exclude_before,exclude_after+1)[None]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i, (x, y) in enumerate(zip(neurons[:-1,None], neurons[1:,None])):\n",
    "    if i in set(list(exclude_idx)):\n",
    "        continue\n",
    "    else:\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "X = np.vstack(X)\n",
    "Y = np.vstack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrcoef(x):\n",
    "    # calculate covariance matrix of rows\n",
    "    mean_x = torch.mean(x, 1, keepdim=True)\n",
    "    xm = x.sub(mean_x.expand_as(x))\n",
    "    c = xm.mm(xm.t())\n",
    "    c = c / (x.size(1) - 1)\n",
    "\n",
    "    # normalize covariance matrix\n",
    "    d = torch.diag(c)\n",
    "    stddev = torch.pow(d, 0.5)\n",
    "    c = c.div(stddev.expand_as(c)+1e-8)\n",
    "    c = c.div(stddev.expand_as(c).t()+1e-8)\n",
    "\n",
    "    # clamp between -1 and 1\n",
    "    # probably not necessary but numpy does it\n",
    "    c = torch.clamp(c, -1.0, 1.0)\n",
    "\n",
    "    return c\n",
    "\n",
    "def idx_permute(start, theEnd, exclude_idx):\n",
    "    num_before_start = np.sum(exclude_idx<start)\n",
    "    num_before_end = np.sum(exclude_idx<theEnd)\n",
    "    return start - num_before_start, theEnd - num_before_end\n",
    "\n",
    "# idx_permute(1,10,np.arange(0,0))\n",
    "\n",
    "class Granger(nn.Module):\n",
    "    def __init__(self, nfeatures, n_past_steps, std=0.1, dtype=T.float32,bias=True):\n",
    "        # we will perform nfeatures x nfeatures regressions\n",
    "        super(Granger, self).__init__()\n",
    "        if dtype==T.float32:\n",
    "            tensor = T.cuda.FloatTensor\n",
    "        elif dtype==T.float16:\n",
    "            tensor = T.cuda.HalfTensor\n",
    "        # Granger coefficient\n",
    "        self.G = nn.Parameter(tensor(n_past_steps,nfeatures,nfeatures).normal_(std),\n",
    "                              requires_grad=True)\n",
    "        self.auto = nn.Parameter(tensor(n_past_steps,nfeatures,nfeatures).normal_(std),\n",
    "                              requires_grad=True)\n",
    "        self.bias = bias\n",
    "        # Bias matrix\n",
    "        if self.bias:\n",
    "            self.B = nn.Parameter(tensor(nfeatures, nfeatures).normal_(std),requires_grad=True)\n",
    "\n",
    "        self.n_past_steps = n_past_steps\n",
    "        self.tensor = tensor\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, x_true, n_past_steps=None):\n",
    "        if n_past_steps==None:\n",
    "            n_past_steps = self.n_past_steps\n",
    "        # nfeatures is n neurons & n_past_steps is lag\n",
    "        # adding dim to make broadcast work\n",
    "        # x_true is batch x lag x nfeatures x         1\n",
    "        # G is          1 x lag x nfeatures x nfeatures\n",
    "        y = self.G[None] * x_true[:,:,:,None]+ self.auto[None] * x_true[:,:,None,:]\n",
    "        # sum lag so y is batch x nfeatures x nfeatures\n",
    "        if self.bias:\n",
    "            return y.sum(1) + self.B[None]\n",
    "        else:\n",
    "            return y.sum(1)\n",
    "\n",
    "def train(model,data,nepochs=1, l1p=1e-8, l2=0, lr=1e-3,batch_size=16, verbose=True):\n",
    "    # l1 per parameter\n",
    "    l1 = l1p / np.prod(T.tensor(model.G.shape).numpy())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    if verbose:\n",
    "        epochs = range(nepochs)\n",
    "    else:\n",
    "        loader = dataloader\n",
    "        epochs = tqdm(range(nepochs))\n",
    "    for epoch in epochs:\n",
    "        if verbose:\n",
    "            loader = tqdm(dataloader)\n",
    "        for X,Y in loader:\n",
    "            Y_pred = model.forward(X)\n",
    "            l = 0\n",
    "#             for param in model.parameters():\n",
    "#                 l += l1* param.norm(1) + l2 *param.norm(2)\n",
    "            if model.bias:\n",
    "                l = l1*model.G.norm(1) + l1*model.B.norm(1)\n",
    "            else:\n",
    "                l = l1*model.G.norm(1)\n",
    "            loss = F.mse_loss(Y[:,None,:], Y_pred) + l\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()# back props\n",
    "            optimizer.step()# update the parameters\n",
    "        if verbose:\n",
    "            print('epoch {}, loss {}'.format(epoch,loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for t in [0,2000]: #np.arange(1600,2800,200):\n",
    "    times.append(slice(*idx_permute(t,t+400, exclude_idx)))\n",
    "times \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Granger(len(neuron_ids),1,bias=False)\n",
    "Gs = np.zeros([len(times),X.shape[1],X.shape[1]])\n",
    "\n",
    "l1 = 5e-6\n",
    "for it,ind in enumerate(times):\n",
    "    model = Granger(X.shape[1],1,bias=False)\n",
    "    data = T.utils.data.TensorDataset(T.from_numpy(X[ind,None]).cuda(),\n",
    "                                  T.from_numpy(Y[ind]).cuda())\n",
    "    train(model,data,3,l1p=1e-0,lr=1e-1,batch_size=1,verbose=False)\n",
    "    train(model,data,15,l1p=5e-2,lr=1e-3,batch_size=1,verbose=False)\n",
    "    Gs[it] = model.G.data[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1,id2 = np.indices(Gs.shape[1:])\n",
    "idx = (np.argmax(np.abs(Gs),axis=0), id1, id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = T.from_numpy(Gs[idx]).cuda()\n",
    "G = T.from_numpy(Gs).cuda().mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_cycle(i):\n",
    "    i = i - 1\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "    color = cmap(i%10)\n",
    "    if i%10==7:\n",
    "        color = (color[0],color[1],color[2]+0.4)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_corr = corrcoef(G).cpu().numpy()\n",
    "\n",
    "row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(row_corr), method='ward')\n",
    "\n",
    "col_corr = corrcoef(model.G[0].t().detach()).cpu().numpy()\n",
    "col_linkage = hierarchy.linkage(\n",
    "    distance.pdist(col_corr.T), method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nclust = 20\n",
    "row_clusters = hierarchy.fcluster(row_linkage,g_nclust,criterion='maxclust')\n",
    "col_clusters = hierarchy.fcluster(col_linkage,g_nclust,criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sb.clustermap(G.cpu().numpy(), row_linkage=row_linkage, row_colors=[cm_cycle(c) for c in row_clusters],\n",
    "        col_linkage=col_linkage, col_colors=[cm_cycle(c) for c in col_clusters],\n",
    "        figsize=(20, 20),cmap=\"RdBu_r\", vmin=-0.1,vmax=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z = [0,2,4,6,8,10]\n",
    "Z = [4]\n",
    "nZ = len(Z)\n",
    "back_img = []\n",
    "for z in Z:\n",
    "    back_img.append(np.power(f.get_tif_rasl_as_vol(z,range(1,200)).mean(axis=2),.4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_clust_to_plot = [17]\n",
    "col_clust_to_plot = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(row_clust_to_plot)\n",
    "nZ = 1\n",
    "plt.subplots(nrows,nZ, figsize=[8*nZ,4*nrows])\n",
    "# for c in range(g_nclust):\n",
    "for ic,clust in enumerate(row_clust_to_plot):\n",
    "    for iz, z in enumerate(Z):\n",
    "        #Select rois in raphe in this slices, and get their coordinates.\n",
    "        coords = df[(row_clusters==clust)].coords\n",
    "#         poly_coords = df[(row_clusters==clust) & (df.z==z)].poly\n",
    "#         poly_coords = df[(row_clusters==clust)].poly\n",
    "#         coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "        plt.subplot(nrows,nZ,ic*nZ+iz+1)\n",
    "        #Overlay the ROIs on the background image and display:\n",
    "        # hack iz hardcode\n",
    "        img = vizutil.overlay_coords(back_img[iz], coords, list(cm_cycle(clust)[:3]), alpha=1)\n",
    "        plt.imshow(img,interpolation='nearest')\n",
    "        plt.title(\"Row cluster {}, z={}\".format(clust,z),fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(col_clust_to_plot)\n",
    "plt.subplots(nrows,nZ, figsize=[8*nZ,4*nrows])\n",
    "# for c in range(g_nclust):\n",
    "for ic,clust in enumerate(col_clust_to_plot):\n",
    "    for iz, z in enumerate(Z):\n",
    "        #Select rois in raphe in this slices, and get their coordinates.\n",
    "#         coords = df[(g_clusters==clust) & (df.z==z)].coords\n",
    "#         poly_coords = df[(row_clusters==clust) & (df.z==z)].poly\n",
    "        coords = df[(col_clusters==clust)].coords\n",
    "#         poly_coords = df[(col_clusters==clust)].poly\n",
    "#         coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "        plt.subplot(nrows,nZ,ic*nZ+iz+1)\n",
    "        #Overlay the ROIs on the background image and display:\n",
    "        img = vizutil.overlay_coords(back_img[iz], coords, list(cm_cycle(clust)[:3]), alpha=1)\n",
    "        plt.imshow(img,interpolation='nearest')\n",
    "        plt.title(\"Col cluster {}, z={}\".format(clust,z),fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "e = 440\n",
    "fig, ax = plt.subplots(1,2, figsize=(5,100))\n",
    "ax[0].imshow(X[slice(*idx_permute(s,e,exclude_idx))].T)\n",
    "ax[1].imshow(x_fish[s:e].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colidx = col_clusters==34\n",
    "rowidx = row_clusters==7\n",
    "plt.matshow(G[rowidx][:,colidx],vmin=-0.1,vmax=0.1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = slice(*idx_permute(2400,2800, exclude_idx))\n",
    "# ind = slice(*idx_permute(0,400, exclude_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Granger(len(neuron_ids),1,bias=False)\n",
    "model = Granger(X.shape[1],1,bias=False)\n",
    "G = model.G.data[0].detach().cpu().numpy()\n",
    "# trunc_fish = x_fish[0:400]\n",
    "# trunc_fish = x_fish[440:840]\n",
    "# trunc_fish = x_fish[840:1240]\n",
    "# trunc_fish = x_fish[1240:1640]\n",
    "# trunc_fish = x_fish[1640:2040]\n",
    "# trunc_fish = x_fish[1640:2440]\n",
    "# trunc_fish = x_fish[640:840]\n",
    "# trunc_fish = x_fish[2400:2800]\n",
    "# trunc_fish = x_fish[2600:2800]\n",
    "# trunc_fish = x_fish\n",
    "# data = T.utils.data.TensorDataset(T.from_numpy(X[ind,None,neuron_ids]).cuda(),\n",
    "#                                   T.from_numpy(Y[ind,neuron_ids]).cuda())\n",
    "data = T.utils.data.TensorDataset(T.from_numpy(X[ind,None]).cuda(),\n",
    "                                  T.from_numpy(Y[ind]).cuda())\n",
    "l1 = 5e-6\n",
    "# train(model,data,10,l1=l1,batch_size=4) # max(bz)==4 for all neurons\n",
    "train(model,data,3,l1p=1e-0,lr=1e-1,batch_size=1,verbose=False)\n",
    "train(model,data,15,l1p=5e-2,lr=1e-3,batch_size=1,verbose=False)\n",
    "G = model.G.data[0].detach().cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p27)",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
