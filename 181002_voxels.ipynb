{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster.bicluster import SpectralBiclustering\n",
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "from sklearn.metrics import consensus_score\n",
    "\n",
    "import scipy.linalg\n",
    "from functools import partial\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sparse\n",
    "from scipy import stats\n",
    "from __future__ import print_function\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from pandas import DataFrame\n",
    "\n",
    "import os, sys, datetime\n",
    "LF_CODE_PATH = os.path.expanduser('~/projects/LFAnalyze/code')\n",
    "FT_CODE_PATH = os.path.expanduser('~/projects/fishTrax/code/analysis/')\n",
    "FD_CODE_PATH = os.path.expanduser('~/projects/fish_despair_notebooks/src/')\n",
    "sys.path.insert(0,LF_CODE_PATH)\n",
    "sys.path.insert(0,FT_CODE_PATH)\n",
    "sys.path.insert(0,FD_CODE_PATH)\n",
    "\n",
    "import passivity_2p_imaging_utils as p2putils\n",
    "import visualization_utils as vizutil\n",
    "reload(p2putils)\n",
    "tmp_dir = '/tmp/'\n",
    "all_data = p2putils.get_all_datasets(tmp_dir=tmp_dir)\n",
    "\n",
    "f = all_data['e'][2]\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "# df, sig = f.get_cnmf_roi_table_and_signals()\n",
    "M = f.get_signals_raw(z=None)\n",
    "\n",
    "# data = np.load(\"../cnmf_f01555.npz\")\n",
    "# cnmf = data['cnmf'].astype(np.float32)\n",
    "# raw = data['raw'].astype(np.float32)\n",
    "# del data\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    # a is a signal\n",
    "    ret = np.cumsum(a,0) # sum over time\n",
    "    ret[n:] = ret[n:] - ret[:-n] # diff of n samples back\n",
    "    rm = ret[n - 1:] / n\n",
    "    pad_start = np.full((n-1,rm.shape[1]), rm[0])\n",
    "    return np.vstack([pad_start, rm])\n",
    "\n",
    "def ewma(data,span):\n",
    "    \"exponential weighted moving average.\"\n",
    "    df = DataFrame(data)\n",
    "    return df.ewm(span).mean().values\n",
    "\n",
    "def df_f(x,ma_window=6,span=6):\n",
    "    u = moving_average(x,ma_window)\n",
    "    return ewma((x - u)/u, span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neurons = sig.T\n",
    "neurons = M.T\n",
    "\n",
    "# neuron_ids = np.sort(np.argsort(neurons.std(0))[-5000:])\n",
    "df = f.get_roi_table()\n",
    "# df = df.iloc[neuron_ids]\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# neurons = neurons[:,neuron_ids]\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "neurons = signal.decimate(neurons,4, axis=0)\n",
    "\n",
    "neurons = df_f(neurons).astype(np.float32)\n",
    "neurons = (neurons - neurons.mean(0))/(neurons.std(0)+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_set = ['l_LHb','r_LHb']\n",
    "\n",
    "#get boolean ndx of all rois not in regions set.\n",
    "df = f.get_roi_table()\n",
    "bndx = np.ones(len(df),dtype=np.bool)\n",
    "for reg in reg_set:\n",
    "    bndx = bndx & ~df['in_'+reg]\n",
    "\n",
    "#get the x,y,z location of all neurons (units um)\n",
    "x = np.array(df.centroid_x) * f.um_per_pixel\n",
    "y = np.array(df.centroid_y) * f.um_per_pixel\n",
    "z = np.array(df.z) * f.z_spacing_um\n",
    "\n",
    "#cube size (um)\n",
    "cube_size = [50,50,30]\n",
    "\n",
    "#take origin as point between the hb.\n",
    "x0 = x[df['in_l_LHb'] | df['in_r_LHb']].mean()\n",
    "y0 = y[df['in_l_LHb'] | df['in_r_LHb']].mean()\n",
    "z0 = z[df['in_l_LHb'] | df['in_r_LHb']].mean()\n",
    "\n",
    "#get edges of boxes to be considered\n",
    "x_edges = np.hstack([np.arange(x0,-cube_size[0],-cube_size[0])[::-1],np.arange(x0,x.max()+cube_size[0],cube_size[0])[1:]])\n",
    "y_edges = np.hstack([np.arange(y0,-cube_size[1],-cube_size[1])[::-1],np.arange(y0,y.max()+cube_size[1],cube_size[1])[1:]])\n",
    "z_edges = np.hstack([np.arange(z0,-cube_size[2],-cube_size[2])[::-1],np.arange(z0,z.max()+cube_size[2],cube_size[2])[1:]])\n",
    "\n",
    "#build list of roi_sets with a box of rois held out from each set.  the first element has no rois heldout\n",
    "bndx_sets = []\n",
    "bndx_sets.append(bndx)\n",
    "\n",
    "import itertools\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = itertools.tee(iterable)\n",
    "    next(b, None)\n",
    "    return itertools.izip(a, b)\n",
    "\n",
    "for xs,xe in pairwise(x_edges):\n",
    "    for ys,ye in pairwise(y_edges):\n",
    "        for zs,ze in pairwise(z_edges):\n",
    "            bndx_sets.append(bndx & ~((x>xs) & (x<xe) & (y>ys) & (y<ye) & (z>zs) & (z<ze)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nT, N = neurons.shape\n",
    "Y_mask = np.ones([2,N], dtype=bool)\n",
    "Y_mask[0] = df['in_'+reg_set[0]]\n",
    "Y_mask[1] = df['in_'+reg_set[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 1\n",
    "y = neurons[lag:,Y_mask[0]]\n",
    "x_null = np.concatenate([neurons[0:-lag], np.ones([neurons.shape[0]-lag,1], dtype=np.float32)],1)\n",
    "nbeta, n_residuals, _, _ = np.linalg.lstsq(x_null, y)\n",
    "n_residuals = ((y - np.matmul(x_null,nbeta))**2).sum()\n",
    "n_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bndx_granger(neurons, Y_mask, X_mask, lag=1):\n",
    "    try:\n",
    "        nT, N = neurons.shape\n",
    "        y = neurons[lag:,Y_mask]\n",
    "        x_granger = np.concatenate([neurons[0:-lag,X_mask], np.ones([nT-lag,1], dtype=np.float32)],1)\n",
    "        gbeta, g_residuals, _, _ = np.linalg.lstsq(x_granger, y)\n",
    "        g_residuals = ((y - np.matmul(x_null,nbeta))**2).sum()\n",
    "        return g_residuals\n",
    "    except:\n",
    "        return \"Error\"\n",
    "    \n",
    "g_residuals = Parallel(n_jobs=32)(delayed(bndx_granger)(neurons, y_mask, x_mask) for (y_mask, x_mask) in product(Y_mask, bndx_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_bndx_batches(bndx_sets, reg_set, X, lag=1, bias=True):\n",
    "    \"\"\"constructs  len(bndx_sets) x len(reg_set) x nT-lag x nBeta. [i, j] index is granger model of influence from i -> j.\n",
    "    \"\"\"\n",
    "    nT, N = X.shape\n",
    "    bz = len(bndx_sets)\n",
    "    N_regions = len(reg_set)\n",
    "    granger = np.ones([bz, N_regions, nT-lag, N+bias])\n",
    "    null = np.ones([bz, N_regions, nT-lag, N+bias])\n",
    "    newY = np.ones([bz, N_regions, nT-lag, 1])\n",
    "    Y = X[lag:]\n",
    "    # time x neuron x lag\n",
    "    l = 0\n",
    "    X_lag = np.concatenate([X[l:-(lag-l)][:,:,None] for l in reversed(range(lag))], 2)\n",
    "    biasTerm = np.ones([nT-lag,1])\n",
    "    for i, n1 in enumerate(range(startIdx,min(N,endIdx))):\n",
    "        for j, n2 in enumerate(range(N)):\n",
    "            if bias:\n",
    "#                 print([x.shape for x in [X_lag[:,n1], X_lag[:,n2], biasTerm]])\n",
    "                grow = np.concatenate([X_lag[:,n1], X_lag[:,n2], biasTerm], 1)\n",
    "                nrow = np.concatenate([X_lag[:,n2], biasTerm], 1)\n",
    "            else:\n",
    "                grow = np.concatenate([X_lag[:,n1], X_lag[:,n2]], 1)\n",
    "                nrow = np.concatenate([X_lag[:,n2]], 1)\n",
    "#             print(i,j)\n",
    "            granger[i*N+j] = grow\n",
    "            null[i*N+j] = nrow\n",
    "            newY[i*N+j] = Y[:,[n2]]\n",
    "    return newY, granger, null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pairwise_batches(X, lag=1, startIdx=None, endIdx=None, bias=True):\n",
    "    \"\"\"constructs batch x nT-lag x nBeta. i*ncol+j index is granger model of influence from i->j.\n",
    "    \n",
    "    Use startIdx and endIdx to choose a certain number of rows,\n",
    "    eg startIdx=0, endIdx=5, X.shape[1]=100 would be a batch of 100^2=10,000.\n",
    "    \n",
    "    WARNING: if endIdx>N, will fill with 1s so batch stays same size.\"\"\"\n",
    "    \n",
    "    nT, N = X.shape\n",
    "    if endIdx:\n",
    "        bz = (endIdx - startIdx) * N\n",
    "    else:\n",
    "        startIdx=0\n",
    "        endIdx=N\n",
    "        bz = N**2\n",
    "#     print([bz, nT-lag, 2*lag+bias])\n",
    "    granger = np.ones([bz, nT-lag, 2*lag+bias])\n",
    "    null = np.ones([bz, nT-lag, lag+bias])\n",
    "    newY = np.ones([bz, nT-lag, 1])\n",
    "    Y = X[lag:]\n",
    "    # time x neuron x lag\n",
    "    l = 0\n",
    "    X_lag = np.concatenate([X[l:-(lag-l)][:,:,None] for l in reversed(range(lag))], 2)\n",
    "    biasTerm = np.ones([nT-lag,1])\n",
    "    for i, n1 in enumerate(range(startIdx,min(N,endIdx))):\n",
    "        for j, n2 in enumerate(range(N)):\n",
    "            if bias:\n",
    "#                 print([x.shape for x in [X_lag[:,n1], X_lag[:,n2], biasTerm]])\n",
    "                grow = np.concatenate([X_lag[:,n1], X_lag[:,n2], biasTerm], 1)\n",
    "                nrow = np.concatenate([X_lag[:,n2], biasTerm], 1)\n",
    "            else:\n",
    "                grow = np.concatenate([X_lag[:,n1], X_lag[:,n2]], 1)\n",
    "                nrow = np.concatenate([X_lag[:,n2]], 1)\n",
    "#             print(i,j)\n",
    "            granger[i*N+j] = grow\n",
    "            null[i*N+j] = nrow\n",
    "            newY[i*N+j] = Y[:,[n2]]\n",
    "    return newY, granger, null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_value(resA, resB, n_params_A, n_params_B, n):\n",
    "    return ( (resA-resB)/(n_params_B-n_params_A) ) / (resB/(n-n_params_B))\n",
    "\n",
    "class Granger():\n",
    "    def __init__(self, x_granger_shape, x_null_shape, y_shape, sess, l2=0.0):\n",
    "        self.x_granger = tf.placeholder(dtype=tf.float32, shape=x_granger_shape)\n",
    "        self.x_null = tf.placeholder(dtype=tf.float32, shape=x_null_shape)\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=y_shape)\n",
    "        self.n_params_granger = x_granger_shape[-1]\n",
    "        self.n_params_null = x_null_shape[-1]\n",
    "        self.l2 = l2\n",
    "        self.sess = sess\n",
    "        \n",
    "    def granger(self, X, Y):\n",
    "        gbeta = tf.matrix_solve_ls(self.x_granger, self.y, l2_regularizer=self.l2, fast=False)\n",
    "        g_residuals = tf.reduce_sum(((tf.matmul(self.x_granger,gbeta) - self.y)**2)[:,:,0], 1)\n",
    "        gbeta, g_residuals = self.sess.run([gbeta, g_residuals],\n",
    "                                      feed_dict={self.x_granger: X, self.y: Y})\n",
    "        return gbeta, g_residuals\n",
    "    \n",
    "    def null(self, X, Y):\n",
    "        gbeta = tf.matrix_solve_ls(self.x_null, self.y, l2_regularizer=self.l2, fast=False)\n",
    "        g_residuals = tf.reduce_sum(((tf.matmul(self.x_null,gbeta) - self.y)**2)[:,:,0], 1)\n",
    "        gbeta, g_residuals = self.sess.run([gbeta, g_residuals],\n",
    "                                      feed_dict={self.x_null: X, self.y: Y})\n",
    "        return gbeta, g_residuals\n",
    "\n",
    "def pairwise_granger_f_val(neurons, lag=1, bz=10):\n",
    "    nT, N = neurons.shape\n",
    "    Y, x_granger, x_null = construct_pairwise_batches(neurons,lag,0,bz)\n",
    "    fvals = np.zeros([N,N])\n",
    "    with tf.Session() as sess:\n",
    "        granger = Granger(x_granger.shape, x_null.shape, Y.shape, sess=sess, l2=0.0)\n",
    "        for start in tqdm(range(0,N,bz)):\n",
    "            end = start+bz\n",
    "            Y, x_granger, x_null = construct_pairwise_batches(neurons,1,start,end)\n",
    "            gbeta, g_residuals = granger.granger(x_granger, Y)\n",
    "            nbeta, n_residuals = granger.null(x_null, Y)\n",
    "            batch_fvals = f_value(g_residuals, n_residuals, granger.n_params_granger, granger.n_params_null, N)\n",
    "            true_end = min(start+bz,N)\n",
    "            batch_fvals = batch_fvals.reshape(-1,N)\n",
    "            if start+bz>N:\n",
    "                batch_true_end = N-start\n",
    "                batch_fvals = batch_fvals[:batch_true_end]\n",
    "            fvals[start:true_end] = batch_fvals\n",
    "    return fvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vals = pairwise_granger_f_val(neurons, 1, 10)\n",
    "\n",
    "plt.imshow(f_vals)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_vals = np.load(\"/data2/trained_models/181002_fvals.npy\")\n",
    "np.save(\"/data2/trained_models/181002_fvals_4x_decimate.npy\",f_vals)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# tf init\n",
    "granger = Granger(x_granger.shape, x_null.shape, Y.shape, l2=0.0)\n",
    "\n",
    "gbeta, g_residuals = granger.granger(x_granger, Y)\n",
    "nbeta, n_residuals = granger.null(x_null, Y)\n",
    "\n",
    "np_beta, _ = np.linalg.lstsq(x_granger[0], Y[0], rcond=None)[0:2]\n",
    "np_res = sum((np.matmul(x_granger[0],np_beta) - Y[0])**2)\n",
    "print(\"tf vs numpy\")\n",
    "print(gbeta[0,:,0],\"\\n\", np_beta[:,0])\n",
    "print(\"residuals\", float(g_residuals[0]), np_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_cycle(i):\n",
    "    i = i - 1\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "    color = cmap(i%10)\n",
    "    if i%10==7:\n",
    "        color = (color[0],color[1],color[2]+0.4)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_corr = np.corrcoef(f_vals)\n",
    "\n",
    "row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(row_corr), method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vals.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_corr = np.corrcoef(f_vals.T)\n",
    "col_linkage = hierarchy.linkage(\n",
    "    distance.pdist(col_corr.T), method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nclust = 10\n",
    "row_clusters = hierarchy.fcluster(row_linkage,g_nclust,criterion='maxclust')\n",
    "col_clusters = hierarchy.fcluster(col_linkage,g_nclust,criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = max(np.abs(np.percentile(f_vals,0.005)), np.abs(np.percentile(f_vals,0.995)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sb.clustermap(f_vals, row_linkage=row_linkage, row_colors=[cm_cycle(c) for c in row_clusters],\n",
    "        col_linkage=col_linkage, col_colors=[cm_cycle(c) for c in col_clusters],\n",
    "        figsize=(20, 20),cmap=\"RdBu_r\", vmin=-vmax,vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(f_vals.reshape(-1), np.linspace(-10,30,50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = [0,2,4,6,8,10]\n",
    "# Z = [4]\n",
    "nZ = len(Z)\n",
    "back_img = []\n",
    "for z in Z:\n",
    "    back_img.append(np.power(f.get_tif_rasl_as_vol(z,range(1,200)).mean(axis=2),.4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = len(row_clust_to_plot)\n",
    "# nZ = 1\n",
    "nrows = g_nclust\n",
    "plt.subplots(nrows,nZ, figsize=[8*nZ,4*nrows])\n",
    "for ic, clust in enumerate(range(1, g_nclust+1)):\n",
    "# for ic,clust in enumerate(row_clust_to_plot):\n",
    "    for iz, z in enumerate(Z):\n",
    "        #Select rois in raphe in this slices, and get their coordinates.\n",
    "#         coords = df[(row_clusters==clust)].coords\n",
    "        coords = df[(row_clusters==clust) & (df.z==z)].coords\n",
    "#         poly_coords = df[(row_clusters==clust) & (df.z==z)].poly\n",
    "#         poly_coords = df[(row_clusters==clust)].poly\n",
    "#         coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "        plt.subplot(nrows,nZ,ic*nZ+iz+1)\n",
    "        #Overlay the ROIs on the background image and display:\n",
    "        # hack iz hardcode\n",
    "        img = vizutil.overlay_coords(back_img[iz], coords, list(cm_cycle(clust)[:3]), alpha=1)\n",
    "        plt.imshow(img,interpolation='nearest')\n",
    "        plt.title(\"Row cluster {}, z={}\".format(clust,z),fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = len(col_clust_to_plot)\n",
    "plt.subplots(nrows,nZ, figsize=[8*nZ,4*nrows])\n",
    "for ic,clust in enumerate(range(1,g_nclust+1)):\n",
    "# for ic,clust in enumerate(col_clust_to_plot):\n",
    "    for iz, z in enumerate(Z):\n",
    "        #Select rois in raphe in this slices, and get their coordinates.\n",
    "        coords = df[(col_clusters==clust) & (df.z==z)].coords\n",
    "#         poly_coords = df[(row_clusters==clust) & (df.z==z)].poly\n",
    "#         coords = df[(col_clusters==clust)].coords\n",
    "#         poly_coords = df[(col_clusters==clust)].poly\n",
    "#         coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "        plt.subplot(nrows,nZ,ic*nZ+iz+1)\n",
    "        #Overlay the ROIs on the background image and display:\n",
    "        img = vizutil.overlay_coords(back_img[iz], coords, list(cm_cycle(clust)[:3]), alpha=1)\n",
    "        plt.imshow(img,interpolation='nearest')\n",
    "        plt.title(\"Col cluster {}, z={}\".format(clust,z),fontsize=18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
