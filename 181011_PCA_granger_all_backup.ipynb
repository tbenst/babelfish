{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster.bicluster import SpectralBiclustering\n",
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "from sklearn.metrics import consensus_score\n",
    "\n",
    "import scipy.linalg\n",
    "from functools import partial\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sparse\n",
    "from scipy import stats\n",
    "from __future__ import print_function\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from pandas import DataFrame\n",
    "\n",
    "import os, sys, datetime\n",
    "LF_CODE_PATH = os.path.expanduser('~/projects/LFAnalyze/code')\n",
    "FT_CODE_PATH = os.path.expanduser('~/projects/fishTrax/code/analysis/')\n",
    "FD_CODE_PATH = os.path.expanduser('~/projects/fish_despair_notebooks/src/')\n",
    "sys.path.insert(0,LF_CODE_PATH)\n",
    "sys.path.insert(0,FT_CODE_PATH)\n",
    "sys.path.insert(0,FD_CODE_PATH)\n",
    "\n",
    "import passivity_2p_imaging_utils as p2putils\n",
    "import visualization_utils as vizutil\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "reload(p2putils)\n",
    "tmp_dir = '/tmp/'\n",
    "all_data = p2putils.get_all_datasets(tmp_dir=tmp_dir)\n",
    "\n",
    "fishIdx = [(\"e\", 2),  (\"e\", 5), (\"c\", 1),  (\"c\", 6),  (\"enp\", 1), (\"enp\", 5)]\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy import signal\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "# df, sig = f.get_cnmf_roi_table_and_signals()\n",
    "\n",
    "\n",
    "# data = np.load(\"../cnmf_f01555.npz\")\n",
    "# cnmf = data['cnmf'].astype(np.float32)\n",
    "# raw = data['raw'].astype(np.float32)\n",
    "# del data\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    # a is a signal\n",
    "    ret = np.cumsum(a,0) # sum over time\n",
    "    ret[n:] = ret[n:] - ret[:-n] # diff of n samples back\n",
    "    rm = ret[n - 1:] / n\n",
    "    pad_start = np.full((n-1,rm.shape[1]), rm[0])\n",
    "    return np.vstack([pad_start, rm])\n",
    "\n",
    "def ewma(data,span):\n",
    "    \"exponential weighted moving average.\"\n",
    "    df = DataFrame(data)\n",
    "    return df.ewm(span).mean().values\n",
    "\n",
    "def df_f(x,ma_window=6,span=6):\n",
    "    u = moving_average(x,ma_window)\n",
    "    return ewma((x - u)/u, span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params:\n",
    "lag = 1\n",
    "bExcludeMotion = False\n",
    "bCNMF = False\n",
    "dff_win = 500\n",
    "decimate = 4\n",
    "bWhite = False\n",
    "bWhiteByWindow = True\n",
    "bTop = False\n",
    "bOneWindow = False\n",
    "nComponents = 1\n",
    "if bOneWindow:\n",
    "    nwindow = 1\n",
    "else:\n",
    "    nwindow = 3\n",
    "\n",
    "fishids = [['f01606','f01604','f01547','f01550','f01553'],#,'f01597','f02326','f01520','f01527',],\n",
    "           ['f01555','f01575','f01576','f01594','f01598'],\n",
    "           ['f01736','f01732','f01733','f01735','f01729']]#,'f\n",
    "# only use CPU for tf\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pairwise_batches(X, lag=1, startIdx=None, endIdx=None, bias=True):\n",
    "    \"\"\"constructs batch x nT-lag x nBeta. i*ncol+j index is granger model of influence from i->j.\n",
    "    \n",
    "    Use startIdx and endIdx to choose a certain number of rows,\n",
    "    eg startIdx=0, endIdx=5, X.shape[1]=100 would be a batch of 100^2=10,000.\n",
    "    \n",
    "    WARNING: if endIdx>N, will fill with 1s so batch stays same size.\"\"\"\n",
    "    \n",
    "    nT, N = X.shape\n",
    "    if endIdx:\n",
    "        bz = (endIdx - startIdx) * N\n",
    "    else:\n",
    "        startIdx=0\n",
    "        endIdx=N\n",
    "        bz = N**2\n",
    "#     print([bz, nT-lag, 2*lag+bias])\n",
    "    granger = np.ones([bz, nT-lag, 2*lag+bias])\n",
    "    null = np.ones([bz, nT-lag, lag+bias])\n",
    "    newY = np.ones([bz, nT-lag, 1])\n",
    "    Y = X[lag:]\n",
    "    # time x neuron x lag\n",
    "    l = 0\n",
    "    X_lag = np.concatenate([X[l:-(lag-l)][:,:,None] for l in reversed(range(lag))], 2)\n",
    "    biasTerm = np.ones([nT-lag,1])\n",
    "    for i, n1 in enumerate(range(startIdx,min(N,endIdx))):\n",
    "        for j, n2 in enumerate(range(N)):\n",
    "            if bias:\n",
    "#                 print([x.shape for x in [X_lag[:,n1], X_lag[:,n2], biasTerm]])\n",
    "                grow = np.concatenate([X_lag[:,n1], X_lag[:,n2], biasTerm], 1)\n",
    "                nrow = np.concatenate([X_lag[:,n2], biasTerm], 1)\n",
    "            else:\n",
    "                grow = np.concatenate([X_lag[:,n1], X_lag[:,n2]], 1)\n",
    "                nrow = np.concatenate([X_lag[:,n2]], 1)\n",
    "#             print(i,j)\n",
    "            granger[i*N+j] = grow\n",
    "            null[i*N+j] = nrow\n",
    "            newY[i*N+j] = Y[:,[n2]]\n",
    "    return newY, granger, null\n",
    "\n",
    "def f_value(resA, resB, n_params_A, n_params_B, n):\n",
    "    return ( (resA-resB)/(n_params_B-n_params_A) ) / (resB/(n-n_params_B))\n",
    "\n",
    "class Granger():\n",
    "    def __init__(self, x_granger_shape, x_null_shape, y_shape, sess, l2=0.0):\n",
    "        self.x_granger = tf.placeholder(dtype=tf.float32, shape=x_granger_shape)\n",
    "        self.x_null = tf.placeholder(dtype=tf.float32, shape=x_null_shape)\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=y_shape)\n",
    "        self.n_params_granger = x_granger_shape[-1]\n",
    "        self.n_params_null = x_null_shape[-1]\n",
    "        self.l2 = l2\n",
    "        self.sess = sess\n",
    "        \n",
    "    def granger(self, X, Y):\n",
    "        gbeta = tf.matrix_solve_ls(self.x_granger, self.y, l2_regularizer=self.l2, fast=False)\n",
    "        g_residuals = tf.reduce_sum(((tf.matmul(self.x_granger,gbeta) - self.y)**2)[:,:,0], 1)\n",
    "        gbeta, g_residuals = self.sess.run([gbeta, g_residuals],\n",
    "                                      feed_dict={self.x_granger: X, self.y: Y})\n",
    "        return gbeta, g_residuals\n",
    "    \n",
    "    def null(self, X, Y):\n",
    "        gbeta = tf.matrix_solve_ls(self.x_null, self.y, l2_regularizer=self.l2, fast=False)\n",
    "        g_residuals = tf.reduce_sum(((tf.matmul(self.x_null,gbeta) - self.y)**2)[:,:,0], 1)\n",
    "        gbeta, g_residuals = self.sess.run([gbeta, g_residuals],\n",
    "                                      feed_dict={self.x_null: X, self.y: Y})\n",
    "        return gbeta, g_residuals\n",
    "\n",
    "def pairwise_granger_f_val(neurons, lag=1, bz=10):\n",
    "    \"all_gbeta is granger then autocorr then bias.\"\n",
    "    nT, N = neurons.shape\n",
    "    Y, x_granger, x_null = construct_pairwise_batches(neurons,lag,0,bz)\n",
    "    fvals = np.zeros([N,N])\n",
    "    all_gbeta = np.zeros([N,N,2*lag+1])\n",
    "    all_nbeta = np.zeros([N,N,lag+1])\n",
    "    with tf.Session(config=config) as sess:\n",
    "        granger = Granger(x_granger.shape, x_null.shape, Y.shape, sess=sess, l2=0.0)\n",
    "        for start in tqdm(range(0,N,bz)):\n",
    "            end = start+bz\n",
    "            Y, x_granger, x_null = construct_pairwise_batches(neurons,1,start,end)\n",
    "            gbeta, g_residuals = granger.granger(x_granger, Y)\n",
    "            nbeta, n_residuals = granger.null(x_null, Y)\n",
    "            batch_fvals = f_value(g_residuals, n_residuals, granger.n_params_granger, granger.n_params_null, N)\n",
    "            # account for N % batch != 0\n",
    "            true_end = min(start+bz,N)\n",
    "            batch_fvals = batch_fvals.reshape(-1,N)\n",
    "            batch_gbeta = gbeta.reshape(-1,N,2*lag+1)\n",
    "            batch_nbeta = nbeta.reshape(-1,N,lag+1)\n",
    "            if start+bz>N:\n",
    "                batch_true_end = N-start\n",
    "                batch_fvals = batch_fvals[:batch_true_end]\n",
    "                batch_gbeta = batch_gbeta[:batch_true_end]\n",
    "                batch_nbeta = batch_nbeta[:batch_true_end]\n",
    "            fvals[start:true_end] = batch_fvals\n",
    "            all_gbeta[start:true_end] = batch_gbeta\n",
    "            all_nbeta[start:true_end] = batch_nbeta\n",
    "    return fvals, all_gbeta, all_nbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fish_data(indicator):\n",
    "    f = all_data[fishIdx[indicator][0]][fishIdx[indicator][1]]\n",
    "    M = f.get_signals_raw(z=None)\n",
    "    neurons = M.T\n",
    "\n",
    "    # neuron_ids = np.sort(np.argsort(neurons.std(0))[-5000:])\n",
    "    df = f.get_roi_table()\n",
    "    # df = df.iloc[neuron_ids]\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # neurons = neurons[:,neuron_ids]\n",
    "\n",
    "\n",
    "    neurons = signal.decimate(neurons,4, axis=0)\n",
    "\n",
    "    neurons = df_f(neurons).astype(np.float32)\n",
    "    neurons = (neurons - neurons.mean(0))/(neurons.std(0)+1e-8)\n",
    "    return f, neurons, df\n",
    "\n",
    "def get_pca_indicator(indicator):\n",
    "    f, neurons, df = read_fish_data(indicator)\n",
    "    return pca(neurons, df)\n",
    "def plot_pca_indicator(indicator):\n",
    "    region_pca, pca_transform = get_pca_indicator(indicator)\n",
    "    plot_pca(region_pca, f)\n",
    "\n",
    "# [plot_pca_indicator(i) for i in range(6)]\n",
    "# pcas = [get_pca_indicator(i) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_regions = [u'in_l_cerebellum', u'in_r_cerebellum', \n",
    "    u'in_l_vthal', u'in_r_vthal', u'in_l_tectum', u'in_r_tectum', \n",
    "    u'in_l_raphe', u'in_r_raphe', u'in_l_hind', u'in_r_hind', \n",
    "    u'in_l_dthal', u'in_r_dthal', u'in_l_LHb', u'in_r_LHb', \n",
    "    u'in_l_tel', u'in_r_tel', u'in_l_MHb',  u'in_r_MHb']\n",
    "\n",
    "_, _, df = read_fish_data(0)\n",
    "regions = []\n",
    "for r in ortho_regions:\n",
    "    if r in df.columns:\n",
    "        regions.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nregions = len(regions)\n",
    "N = nregions * nComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(neurons, df, n_components=None):\n",
    "    region_pca = {}\n",
    "    for r in regions:\n",
    "        X = neurons[:,df[r]]\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "        transform = pca.fit_transform(X)\n",
    "        region_pca[r] = {\"components\": pca.components_, \"explained_var\": pca.explained_variance_ratio_, \"transform\": transform}\n",
    "    neurons_pca_transform = np.concatenate([region_pca[r]['transform'] for r in regions],1)\n",
    "    return region_pca, neurons_pca_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fvals = np.zeros([15, nwindow, N,N])\n",
    "all_gbeta = np.zeros([15, nwindow, N,N,2*lag+1])\n",
    "all_nbeta = np.zeros([15, nwindow, N,N,lag+1])\n",
    "for ncond, cond in enumerate(['shock','reexposed','control',]):\n",
    "    for nf, fid in enumerate(fishids[ncond]):\n",
    "        f = p2putils.get_fish(all_data,fid)[0]\n",
    "        print('Starting', f.fishid)\n",
    "        \n",
    "        if bExcludeMotion:\n",
    "            exclude_before = 1\n",
    "            exclude_after = 4\n",
    "            motion_frames = numpy.searchsorted(f.frame_et[:,-1], f.tail_movement_start_times,side=\"left\")\n",
    "            exclude_idx = np.unique((motion_frames[:,None] + np.arange(-exclude_before,exclude_after+1)[None]).flatten())\n",
    "        else:\n",
    "            exclude_idx = None\n",
    "            \n",
    "        \n",
    "        #Load the signals\n",
    "        if bCNMF:\n",
    "            df, M = f.get_cnmf_roi_table_and_signals()\n",
    "        else:\n",
    "            M = f.get_signals_raw(z=None)\n",
    "            df = f.get_roi_table()\n",
    "        neurons = M.T  \n",
    "#         print(\"INITIAL SHAPE\", neurons.shape)\n",
    "        #build time windows of frames to examine\n",
    "        if bOneWindow:\n",
    "            windows = [(0,np.argmax(f.frame_st[:,0]>(f.get_shock_start_time() + 60*24 + 360)))]\n",
    "        else:\n",
    "            window_len = 360\n",
    "            windows = [(np.argmax(f.frame_st[:,0]>f.get_shock_start_time()-window_len), np.argmax(f.frame_st[:,0]>f.get_shock_start_time())),\n",
    "                       (np.argmax(f.frame_st[:,0]>f.get_shock_start_time()),np.argmax(f.frame_st[:,0]>f.get_shock_start_time()+window_len)),\n",
    "    #                    (neurons.shape[0]-360,neurons.shape[0])\n",
    "                       (np.argmax(f.frame_st[:,0]>(f.get_shock_start_time() + 60*24)),np.argmax(f.frame_st[:,0]>(f.get_shock_start_time() + 60*24 + window_len))),\n",
    "                       #(np.argmax(f.frame_st[:,0]>f.get_shock_start_time()-window_len), np.argmax(f.frame_st[:,0]>(f.get_shock_start_time() + 60*24 + window_len)))\n",
    "                      ]\n",
    "        \n",
    "        #Extract top 5000 plus all in regions\n",
    "        if bTop:\n",
    "            bndx = np.zeros(df.shape[0],dtype=np.bool)\n",
    "            for nReg, reg in enumerate(regions):\n",
    "                if reg in df.columns:\n",
    "                    bndx = bndx | df[reg]\n",
    "            bndx[np.argsort(neurons.std(0))[-5000:]] = True\n",
    "            neuron_ids = np.where(bndx)[0]\n",
    "            df = df.iloc[neuron_ids]\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            neurons = neurons[:,neuron_ids]\n",
    "        print('num neurons %d'%(neurons.shape[1]))\n",
    "        #Preprocess\n",
    "        if not bCNMF: #if not CNMF loaded above\n",
    "            neurons = df_f(neurons,dff_win,6) #6,6 or 500,6\n",
    "        else: #else if CNMF data\n",
    "            neurons = (neurons - neurons.mean(0))/neurons.mean(0) \n",
    "        if decimate is not None:\n",
    "            neurons = signal.decimate(neurons,decimate,axis=0)\n",
    "            windows = [(int(np.floor(w0/decimate)), int(np.floor(w1/decimate))) for w0,w1 in windows]\n",
    "        if bWhite:\n",
    "            neurons = (neurons - neurons.mean(0))/(neurons.std(0)+1e-8)\n",
    "            ndx = np.where(neurons.std(0)==0)\n",
    "            neurons[:,ndx] = 0\n",
    "#         print(\"AFTER\", neurons.shape)\n",
    "        \n",
    "        for it,window in enumerate(windows):\n",
    "            wneurons = neurons[window[0]:window[1]]\n",
    "            if bWhiteByWindow:\n",
    "                wneurons = (wneurons - wneurons.mean(0))/(wneurons.std(0)+1e-8) #numericall reversible?\n",
    "                ndx = np.where(wneurons.std(0)==0)\n",
    "                wneurons[:,ndx] = 0\n",
    "            # RUN ANALYSIS HERE\n",
    "#             print(wneurons.shape)\n",
    "            region_pca, neurons_pca_transform = pca(wneurons, df, nComponents)\n",
    "            fvals, gbeta, nbeta = pairwise_granger_f_val(\n",
    "                neurons_pca_transform, 1, 10)\n",
    "            all_fvals[nf,it] = fvals\n",
    "            all_gbeta[nf,it] = gbeta\n",
    "            all_nbeta[nf,it] = nbeta\n",
    "        # SAVE DATA\n",
    "#         fn = f.data_prefix+'_pca.npz'\n",
    "#         np.savez(fn, all_pcas[cond][fid][window])\n",
    "        print('Done', f.fishid)#, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = list(map(lambda x: x[3:],regions))\n",
    "plt.figure(figsize=(30,100))\n",
    "fish_ids_str = np.array(fishids).flatten()\n",
    "for f in range(15):\n",
    "    for w in range(3):\n",
    "        plt.subplot(15,3,f*3+w+1)\n",
    "        plt.imshow(all_fvals[f,w],vmin=0,vmax=3.5)\n",
    "        plt.colorbar()\n",
    "        plt.xticks(np.arange(len(reg)), reg)\n",
    "        plt.yticks(np.arange(len(reg)), reg)\n",
    "        plt.title(\"{} F-value, window {},{} PCA\".format(\n",
    "            fish_ids_str[f], w, nComponents))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fish_data(indicator):\n",
    "    f = all_data[fishIdx[indicator][0]][fishIdx[indicator][1]]\n",
    "    M = f.get_signals_raw(z=None)\n",
    "    neurons = M.T\n",
    "\n",
    "    # neuron_ids = np.sort(np.argsort(neurons.std(0))[-5000:])\n",
    "    df = f.get_roi_table()\n",
    "    # df = df.iloc[neuron_ids]\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # neurons = neurons[:,neuron_ids]\n",
    "\n",
    "\n",
    "    neurons = signal.decimate(neurons,4, axis=0)\n",
    "\n",
    "    neurons = df_f(neurons).astype(np.float32)\n",
    "    neurons = (neurons - neurons.mean(0))/(neurons.std(0)+1e-8)\n",
    "    return f, neurons, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_letter(x):\n",
    "    return x[0][5]\n",
    "def plot_pca(region_pca, f):\n",
    "    N = len(region_pca)\n",
    "    iterator = enumerate(sorted(region_pca.items(), key=region_letter))\n",
    "    plt.figure()\n",
    "    for i, (region, data) in iterator:\n",
    "        nNeurons = sum(df[region])\n",
    "        var = np.cumsum(data['explained_var'])\n",
    "        sns.lineplot(nNeurons/np.arange(1,len(var)+1),var, label=region,\n",
    "                     color=sns.color_palette('tab20')[i])\n",
    "    plt.xlabel(\"Compression ( # neurons / # PCs)\")\n",
    "    plt.ylabel(\"Explained variance\")\n",
    "    plt.xlim(0,100)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f.fishid)\n",
    "\n",
    "def plot_pca_over_neurons(region_pca, f):\n",
    "    N = len(region_pca)\n",
    "    iterator = enumerate(sorted(region_pca.items(), key=region_letter))\n",
    "    plt.figure()\n",
    "    for i, (region, data) in iterator:\n",
    "        nNeurons = sum(df[region])\n",
    "        var = np.cumsum(data['explained_var'])\n",
    "        sns.lineplot(np.arange(1,len(var)+1)/nNeurons,var, label=region,\n",
    "                     color=sns.color_palette('tab20')[i])\n",
    "    plt.xlabel(\"Compression ( # PCs / # neurons)\")\n",
    "    plt.ylabel(\"Explained variance\")\n",
    "#     plt.xlim(0,100)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f.fishid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_indicator(indicator):\n",
    "    f, neurons, df = read_fish_data(indicator)\n",
    "    return pca(neurons, df)\n",
    "def plot_pca_indicator(indicator):\n",
    "    region_pca, pca_transform = get_pca_indicator(indicator)\n",
    "    plot_pca(region_pca, f)\n",
    "\n",
    "# [plot_pca_indicator(i) for i in range(6)]\n",
    "pcas = [get_pca_indicator(i) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem(np.vstack([np.arange(5),np.arange(5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sem_pcas(pcas):\n",
    "    avg_sem = {}\n",
    "    for (region_pca, pca_transform) in pcas:\n",
    "        iterator = enumerate(sorted(region_pca.items(), key=region_letter))\n",
    "        for i, (region, data) in iterator:\n",
    "            var = np.cumsum(data['explained_var'])\n",
    "            if region not in avg_sem:\n",
    "                avg_sem[region] = []\n",
    "            avg_sem[region].append(var)\n",
    "    for region,v in avg_sem.items():\n",
    "        l = min([x.shape[0] for x in v])\n",
    "        stack = np.vstack([x[:l] for x in v])\n",
    "#         avg_sem[region] = (stack.mean(0), sem(stack))\n",
    "        avg_sem[region] = stack\n",
    "    return avg_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sem_pcas(pcas):\n",
    "    avg_sem = pd.DataFrame(columns=[\"fish_id\", \"region\", \"pca_num\", \"explained_var\"])\n",
    "    for (region_pca, pca_transform) in pcas:\n",
    "        iterator = enumerate(sorted(region_pca.items(), key=region_letter))\n",
    "        for i, (region, data) in iterator:\n",
    "            var = np.cumsum(data['explained_var'])\n",
    "            df = pd.DataFrame({\"fish_id\": i, \"region\": region, \"pca_num\": np.arange(len(var)), \"explained_var\":var})\n",
    "            avg_sem = avg_sem.append(df)\n",
    "    return avg_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sem = avg_sem_pcas(pcas)\n",
    "avg_sem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=\"pca_num\", y=\"explained_var\", hue=\"region\", data=avg_sem,\n",
    "                  palette=sns.color_palette('tab20')[:len(avg_sem[\"region\"].unique())])\n",
    "ax.set_xlim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(\"pca_by_fish.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_over_neurons(region_pca, f):\n",
    "    N = len(region_pca)\n",
    "    iterator = enumerate(sorted(region_pca.items(), key=region_letter))\n",
    "    plt.figure()\n",
    "    for i, (region, data) in iterator:\n",
    "        nNeurons = sum(df[region])\n",
    "        var = np.cumsum(data['explained_var'])\n",
    "        sns.lineplot(np.arange(1,len(var)+1)/nNeurons,var, label=region,\n",
    "                     color=sns.color_palette('tab20')[i])\n",
    "    plt.xlabel(\"Compression ( # PCs / # neurons)\")\n",
    "    plt.ylabel(\"Explained variance\")\n",
    "#     plt.xlim(0,100)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f.fishid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
