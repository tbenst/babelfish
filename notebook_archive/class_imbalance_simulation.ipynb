{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook aims to test the ability of neural network to esimate probabilities when probabilities are low and when samples are not evenly distributed.  In other word when many requests come from cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key open questions  \n",
    "0. Does it matter if world_size is 1 or 100?  Aka do I need to preprocess\n",
    "1. Does downsmapling help with p fit?\n",
    "2. Does downsampling bias results? Can they be un-biased?\n",
    "3. Do deeper or wider networks help?\n",
    "4. Do SELUs help?\n",
    "5. Do gaussian blob regularized to prevent small blobs, yield better fits?\n",
    "6. Which batch sizes work best?\n",
    "7. What is best way to prevent overfitting?\n",
    "8. Does it help to gradually reduce learning rate... create new optimizer ever N epochs \n",
    "\n",
    "TODO: \n",
    "1. fix prob map generation: cluster_peaks is not parameterized well.\n",
    "2. Save movie of fitting at finer scale then 1 epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate data  \n",
    "\n",
    "Their are only two features, x and y, which represent location.  \n",
    "\n",
    "The are two maps associated with x,y location.\n",
    "\n",
    "The first is the probabiilty of a request being generated a position x,y. \n",
    "\n",
    "The second is the probabilty of a conversion occuring a position x,y.\n",
    "\n",
    "There are 4 classes generating request locations.  We define the probability that a request comes from each class.  Then each class is 2d gaussian blob, except the last class, which is uniform.\n",
    "\n",
    "The binomial probability of converting at each location is a map formed by summing several multivariate gaussian blobs with distinct locations and shapes from the request generating blobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# matplotlib notebook is better, but not in Jupyter Lab\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters of generated data\n",
    "world_size = (100,100)\n",
    "num_requests = 1000000\n",
    "\n",
    "#Define request distrubiton\n",
    "city_dist = [.2,.15,.1,.5,.05] #city 1, 2, 3, 4, uniform\n",
    "city_locations = [(20,20), (70,50), (30,80), [30,50]]\n",
    "city_covariance = [[[25,0],[0,25]],\n",
    "                   [[20,-15],[-15,20]],\n",
    "                   [[5,3],[3,5]],\n",
    "                   [[200,0],[0,400]]]\n",
    "\n",
    "#Define conversion probabillity\n",
    "convert_baseline = .00001\n",
    "#cluster_prob = [.005,.02,.01,.001,.04]\n",
    "#cluster_prob = [.5,.3,1,.05,.7]\n",
    "cluster_prob = [20,20,20,20,20]\n",
    "cluster_loc = [(20,25), (65,55), (10,60), (20,15), (85,10)]\n",
    "cluster_cov = [[[10,0],[0,20]],\n",
    "               [[20,15],[15,20]],\n",
    "               [[10,0],[0,10]],\n",
    "               [[4,0],[0,4]],\n",
    "               [[4,2],[2,4]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters of generated data\n",
    "world_size = (1,1)\n",
    "num_requests = 1000000\n",
    "\n",
    "#Define request distrubiton\n",
    "city_dist = [.2,.15,.1,.5,.05] #city 1, 2, 3, 4, uniform\n",
    "city_locations = [(.20,.20), (.70,.50), (.30,.80), (.30,.50)]\n",
    "city_covariance = [[[.025,0],[0,.025]],\n",
    "                   [[.020,-.015],[-.015,.020]],\n",
    "                   [[.005,.003],[.003,.005]],\n",
    "                   [[2.00,0],[0,4.00]]]\n",
    "                   \n",
    "#Define conversion probabillity\n",
    "convert_baseline = .00001\n",
    "#cluster_prob = [.005,.02,.01,.001,.04]\n",
    "#cluster_prob = [.5,.3,1,.05,.7]\n",
    "#cluster_prob = [.005,.005,.005,.005,.007]\n",
    "cluster_prob = [.0005,.0003,.001,.00005,.005]\n",
    "cluster_loc = [(.20,.25), (.65,.55), (.10,.60), (.20,.15), (.85,.10)]\n",
    "cluster_cov = [[[.0010,0],[0,.0020]],\n",
    "               [[.0020,.0015],[.0015,.0020]],\n",
    "               [[.0010,0],[0,.0010]],\n",
    "               [[.004,0],[0,.004]],\n",
    "               [[.004,.002],[.002,.004]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate request locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get counts from each city\n",
    "city_counts = np.random.multinomial(num_requests, city_dist)\n",
    "\n",
    "requests = []\n",
    "for n_city in range(len(city_counts)):\n",
    "    if n_city < len(city_dist)-1:\n",
    "        r = np.random.multivariate_normal(city_locations[n_city], \n",
    "                                          city_covariance[n_city],\n",
    "                                          city_counts[n_city])\n",
    "    else:\n",
    "        r = np.random.random([city_counts[n_city],2]) * world_size\n",
    "    requests.append(r)\n",
    "requests = np.vstack(requests)\n",
    "requests = requests[(requests[:,0]>0) & (requests[:,0]<world_size[0]),:]\n",
    "requests = requests[(requests[:,1]>0) & (requests[:,1]<world_size[1]),:]\n",
    "np.random.shuffle(requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of request locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=requests[:,0], y=requests[:,1], kind=\"hex\");\n",
    "plt.xlim(0,world_size[0])\n",
    "plt.ylim(0,world_size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize probablity of conversion map by sampling on grid  \n",
    "`grid` is the set of points to be sampled (will be used later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#visualize the probability map\n",
    "x, y = np.mgrid[0:world_size[0]:world_size[0]/100, 0:world_size[1]:world_size[1]/100]\n",
    "grid = np.dstack((x, y))\n",
    "p_grid = np.ones(grid.shape[0:2])*convert_baseline\n",
    "for nc in range(len(cluster_prob)):\n",
    "    p_grid += cluster_prob[nc] * multivariate_normal.pdf(grid, mean=cluster_loc[nc], cov=cluster_cov[nc])\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.set_aspect(1)\n",
    "ax2.contourf(x, y, np.power(p_grid,.3), 256) #altered gamma so faint blobs visible\n",
    "ax2.set_title('Probability of converting.')\n",
    "plt.xlim(0,world_size[0])\n",
    "plt.ylim(0,world_size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip coin for each request to determine if it converted.  \n",
    "`p_request` is the probability each request converted.  \n",
    "`b_convert` is the result of the coin flips (the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_requests = np.ones(requests.shape[0])*convert_baseline\n",
    "for nc in range(len(cluster_prob)):\n",
    "    p_requests += cluster_prob[nc] * multivariate_normal.pdf(requests, mean=cluster_loc[nc], cov=cluster_cov[nc])\n",
    "b_convert = np.random.random(requests.shape[0])<p_requests\n",
    "\n",
    "print('Num conversions:', b_convert.sum())\n",
    "print('Coversion prob max:', p_requests.max(), 'avg:', p_requests.mean())\n",
    "plt.scatter(requests[b_convert,0],requests[b_convert,1])\n",
    "plt.xlim(0,world_size[0])\n",
    "plt.ylim(0,world_size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = np.floor(len(requests)*9/10).astype(int)\n",
    "X_train, X_test = requests[:split].astype('single'), requests[split:].astype('single')\n",
    "y_train, y_test = b_convert[:split].astype('int32'), b_convert[split:].astype('int32')\n",
    "y_train = np.expand_dims(y_train,1)\n",
    "y_test = np.expand_dims(y_test,1)\n",
    "p_train, p_test = p_requests[:split], p_requests[split:]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Tensor Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1024\n",
    "\n",
    "def get_tf_dataset(X, y, batch_size, bShuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,y))\n",
    "    if bShuffle:\n",
    "        dataset = dataset.shuffle(20000)\n",
    "    #dataset = dataset.repeat() - prevents epoch loops\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "    \n",
    "train_dataset = get_tf_dataset(X_train,y_train,batch_size)\n",
    "test_dataset = get_tf_dataset(X_test,y_test,batch_size)\n",
    "    \n",
    "# View a single example entry from a batch and check shapes\n",
    "features, label = tfe.Iterator(train_dataset).next()\n",
    "print(\"example features:\", features[0])\n",
    "print(\"example label:\", label[0])\n",
    "print(\"features shape and type:\", features.shape, features.dtype)\n",
    "print(\"features shape and type:\", label.shape, label.dtype)\n",
    "\n",
    "#Also create a grid dataset for visualizing models probability map\n",
    "grid_list = grid.reshape(grid.shape[0]*grid.shape[1],grid.shape[2]).astype(np.single)\n",
    "grid_dataset = tf.data.Dataset.from_tensor_slices(grid_list).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(100, activation=\"relu\", input_shape=(2,)),  # input shape required\n",
    "  tf.keras.layers.Dropout(.5),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "  tf.keras.layers.Dropout(.5),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(25, activation=\"relu\"),\n",
    "  tf.keras.layers.Dropout(.5),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(12, activation=\"relu\"),\n",
    "  tf.keras.layers.Dropout(.5),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(2)\n",
    "])\n",
    "def loss(model, x, y):\n",
    "  y_ = model(x)\n",
    "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep results for plotting\n",
    "# seperate cell so can start/stop fitting process (lower learning rate)\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "rmse_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "## Note: Rerunning this cell uses the same model variables\n",
    "\n",
    "n_step=0\n",
    "num_epochs = 750\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3,nrows=1, figsize=(15,5))\n",
    "\n",
    "axes[0].imshow(p_grid, cmap='viridis', extent=[0,world_size[0],0,world_size[1]],vmin=0, vmax=p_grid.max())\n",
    "axes[0].set_title('Target')\n",
    "im_model = axes[1].imshow(p_grid, cmap='viridis', extent=[0,world_size[0],0,world_size[1]],vmin=0, vmax=p_grid.max())\n",
    "axes[1].set_title('Model')\n",
    "im_diff = axes[2].imshow(p_grid-p_grid, cmap='viridis', extent=[0,world_size[0],0,world_size[1]],\n",
    "                         vmin=-.05, vmax=.05)\n",
    "axes[2].set_title('Diff')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tfe.metrics.Mean()\n",
    "    epoch_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "    # Training loop \n",
    "    for x, y in tfe.Iterator(train_dataset):\n",
    "        # Optimize the model\n",
    "        grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                                  global_step=tf.train.get_or_create_global_step())\n",
    "        n_step+=1\n",
    "\n",
    "        # Track progress\n",
    "        l = loss(model,x,y)\n",
    "        epoch_loss_avg(l)  # add current batch loss\n",
    "        # compare predicted label to actual label\n",
    "        epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)\n",
    " \n",
    "    # end epoch\n",
    "    # use model to examine grid fit:\n",
    "    p_model = []\n",
    "    for x in tfe.Iterator(grid_dataset):\n",
    "        p_model.append(tf.nn.softmax(model(x)))\n",
    "    p_model = np.vstack(p_model)\n",
    "    p_model = p_model[:,1].reshape(grid.shape[0],grid.shape[1])\n",
    "    rmse = ((p_model-p_grid)**2).mean()**.5\n",
    "\n",
    "    #record results\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "    rmse_results.append(rmse)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        im_model.set_data(p_model)\n",
    "        im_diff.set_data(p_grid - p_model)\n",
    "        axes[2].set_title(\"Diff RMSE: {:.3f} ColorRange: +-0.05\".format(rmse))\n",
    "        clear_output(wait=True)\n",
    "        display(fig)    \n",
    "    \n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}, RMSE: {:.3f}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_results, label='train-loss')\n",
    "plt.plot(rmse_results, label='rmse')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3,nrows=1, figsize=(15,5))\n",
    "axes[0].imshow(p_grid, cmap='viridis', extent=[0,world_size[0],0,world_size[1]],vmin=0, vmax=p_grid.max())\n",
    "axes[0].set_title('Target')\n",
    "im_model = axes[1].imshow(p_model, cmap='viridis', extent=[0,world_size[0],0,world_size[1]],vmin=0, vmax=p_grid.max())\n",
    "axes[1].set_title('Model')\n",
    "im_diff = axes[2].imshow(p_grid-p_model, cmap='viridis', extent=[0,world_size[0],0,world_size[1]], \n",
    "                         vmin=-np.abs(p_grid-p_model).max(),\n",
    "                         vmax=np.abs(p_grid-p_model).max(),\n",
    "                        )\n",
    "rmse = ((p_model-p_grid)**2).mean()**.5\n",
    "axes[2].set_title(\"Diff RMSE: {:.3f} Range: +-{:.3f}\".format(rmse, np.abs(p_grid-p_model).max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up test datset using requests\n",
    "\n",
    "test_accuracy = tfe.metrics.Accuracy()\n",
    "grid_list = grid.reshape(grid.shape[0]*grid.shape[1],grid.shape[2])\n",
    "\n",
    "test_dataset = get_tf_dataset(X_test,y_test,2056)\n",
    "\n",
    "\n",
    "\n",
    "p_model\n",
    "for (x, y) in test_dataset:\n",
    "  prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\n",
    "  test_accuracy(prediction, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)    \n",
    "\n",
    "p_predicted = clf.predict_proba(X_test)[:,1]\n",
    "print(p_predicted[3906],p_test[3906])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
