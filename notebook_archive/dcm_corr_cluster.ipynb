{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import scipy.linalg\n",
    "from functools import partial\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "plt.ioff()\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sparse\n",
    "from scipy import stats\n",
    "from __future__ import print_function\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import torch as T\n",
    "import torch\n",
    "import os\n",
    "import skvideo\n",
    "from tqdm import tqdm\n",
    "import skvideo.io\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 / np.arange(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FishSeqData(Dataset):    \n",
    "    def __init__(self, u, p, x,n_future_steps=1):\n",
    "        self.x = nn.Parameter(x,requires_grad=False)\n",
    "        self.p = nn.Parameter(p,requires_grad=False)\n",
    "        self.u = nn.Parameter(u,requires_grad=False)\n",
    "        self.nfeatures = x.shape[1]\n",
    "        self.n_future_steps = n_future_steps\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)-self.n_future_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = slice(idx,idx+self.n_future_steps)\n",
    "        x_true_indices = slice(idx+1,idx+self.n_future_steps+1)\n",
    "        return (self.u[indices], self.p[indices],\n",
    "                self.x[indices], self.x[x_true_indices])\n",
    "\n",
    "class DynamicsCluster(nn.Module,):\n",
    "    def __init__(self, nfeatures, neuron_cluster_map, n_future_steps,dtype=T.float32, scale=1,std=0.1):\n",
    "        \"\"\"DCM model with dynamics between nclusters = max(neuron_cluster_map) features and dense mapping from\n",
    "        nfeatures -> nclusters\"\"\"\n",
    "        super(DynamicsSeq, self).__init__()\n",
    "        if dtype==T.float32:\n",
    "            tensor = T.cuda.FloatTensor\n",
    "        elif dtype==T.float16:\n",
    "            tensor = T.cuda.HalfTensor\n",
    "        nclusters = max(neuron_cluster_map)\n",
    "        self.nclusters = nclusters\n",
    "        self.cluster = []\n",
    "        for i in range(nclusters):\n",
    "            idx = np.argwhere(neuron_cluster_map==i)\n",
    "            self.cluster.append(idx)\n",
    "        self.Dense = nn.Parameter(tensor(nfeatures).normal_(std), requires_grad=True)\n",
    "        self.A = nn.Parameter(tensor(nclusters,nclusters).normal_(std),requires_grad=True)\n",
    "        self.B = nn.Parameter(tensor(nclusters,nclusters).normal_(std),requires_grad=True)\n",
    "        self.C = nn.Parameter(tensor(nclusters).normal_(std),requires_grad=True)\n",
    "        self.n_future_steps = n_future_steps\n",
    "        self.tensor = tensor\n",
    "        self.scale = scale\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, u, p, x_true, n_future_steps=None):\n",
    "        if n_future_steps==None:\n",
    "            n_future_steps = self.n_future_steps\n",
    "        x = self.tensor(x_true.shape[0], 1+x_true.shape[1], *x_true.shape[2:]).zero_()\n",
    "        x[:,0] = x_true[:,0]\n",
    "        for t in range(n_future_steps):\n",
    "            # batch x time x feature\n",
    "            X = T.cuda.FloatTensor(x_true.shape[0],self.nclusters, 1).zero_()\n",
    "            for i in range(self.nclusters):\n",
    "                idx = self.cluster[i]\n",
    "                # broadcast inner product\n",
    "                X[:,i] = T.matmul(self.Dense[idx], x_true[:,t,idx,None])\n",
    "            dxdt = (T.matmul((self.A + p[:,t,None,None]*self.B), X).squeeze()) + u[ :,t,None]*self.C\n",
    "            assert dxdt.shape == [x_true.shape[0],self.nclusters, 1]\n",
    "            for i in range(self.nclusters):\n",
    "                idx = self.cluster[i]\n",
    "                x[:,t+1,idx] = dxdt[:,i]/self.Dense[idx] + x[:,t,idx]\n",
    "        return x[:,1:]\n",
    "\n",
    "def train(model,data,nepochs=10, lambdaA=(1e-8, 1e-6), lambdaB=(1e-6, 1e-6),\n",
    "          lambdaC=(1e-5, 1e-5), lambdaD=(1e-5, 1e-5), lr=0.1, verbose=True):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    if verbose:\n",
    "        A_loss = F.mse_loss(model.A.data,A_true)\n",
    "        print(\"A_loss: {}\".format(A_loss))\n",
    "    cum_mse_loss = 0\n",
    "    with T.no_grad():\n",
    "        for batch_data in tqdm(dataloader):\n",
    "            U,P,X, X_true = batch_data\n",
    "            X_pred = model(U,P,X)\n",
    "            cum_mse_loss += F.mse_loss(X_pred.float(),X_true.float())\n",
    "        nfeatures = X_true.shape[2]\n",
    "        print(\"mse_loss: {:3E}\".format(cum_mse_loss/nfeatures))\n",
    "#     optimizer = T.optim.SGD(model.parameters(),lr=lr)\n",
    "    optimizer = T.optim.Adam(model.parameters(),lr=lr,amsgrad=True)\n",
    "    for e in range(nepochs):\n",
    "        if verbose:\n",
    "            print(\"epoch {}: \".format(e), end=\"\")\n",
    "        cum_loss = 0\n",
    "        cum_mse_loss = 0\n",
    "        for batch_data in tqdm(dataloader):\n",
    "            U,P,X, X_true = batch_data\n",
    "            X_pred = model(U,P,X)\n",
    "            l_A = lambdaA[0] * model.A.norm(1) + lambdaA[1] * model.A.norm(2)\n",
    "            l_B = lambdaB[0] * model.B.norm(1) + lambdaB[1] * model.B.norm(2)\n",
    "            l_C = lambdaC[0] * model.C.norm(1) + lambdaC[1] * model.C.norm(2)\n",
    "            l_D1 = lambdaD[0] * model.Dense1.norm(1) + lambdaD[1] * model.Dense1.norm(2)\n",
    "#             l_D2 = lambdaD[0] * model.Dense2.norm(1) + lambdaD[1] * model.Dense2.norm(2)\n",
    "            mse_loss = F.mse_loss(X_pred,X_true)\n",
    "            loss = mse_loss + l_A + l_B + l_C + l_D1 #+ l_D2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cum_loss += float(loss)\n",
    "            cum_mse_loss += float(mse_loss)\n",
    "            del X_pred, U,P,X, X_true, mse_loss, l_A, l_B, loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if verbose:\n",
    "            A_loss = F.mse_loss(model.A.data,A_true)\n",
    "            B_loss = F.mse_loss(model.B.data,B_true)\n",
    "            C_loss = F.mse_loss(model.C.data,C_true)\n",
    "            print(\"pred_loss: {}, A_loss: {}, B_loss: {}, C_loss: {}\".format(cum_loss,A_loss,B_loss,C_loss))\n",
    "        print(\"cum_loss: {:3E}, mse_loss: {:3E}\".format(cum_loss,cum_mse_loss/nfeatures))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"pred_loss: {}, A_loss: {}, B_loss: {}, C_loss: {}\".format(cum_loss,A_loss,B_loss,C_loss))\n",
    "        \n",
    "def predict(model,data, batch_size=32):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    with T.no_grad():\n",
    "        pred = T.cuda.FloatTensor(len(data),*data[0][2].shape).zero_()\n",
    "        for i, batch_data in enumerate(tqdm(dataloader)):\n",
    "            U,P,X, X_true = batch_data\n",
    "            X_pred = model(U,P,X)\n",
    "            pred[i*batch_size:(i+1)*batch_size] = X_pred\n",
    "    return pred\n",
    "\n",
    "\n",
    "def model_v_truth(model,u,p,x_true,A_true=None, B_true=None):\n",
    "    if model.dtype==T.float16:\n",
    "        A = model.A.data.float()*model.scale\n",
    "        B = model.B.data.float()*model.scale\n",
    "    else:\n",
    "        A = model.A.data\n",
    "        B = model.B.data\n",
    "    x_pred = predict(model,data).squeeze()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    spec = gridspec.GridSpec(ncols=2, nrows=3)\n",
    "    anno_opts = dict(xy=(0.5, 0.5), xycoords='axes fraction',\n",
    "                     va='center', ha='center')\n",
    "\n",
    "    ax1 = fig.add_subplot(spec[0,0:])\n",
    "    ax2 = fig.add_subplot(spec[1, 0])\n",
    "    ax3 = fig.add_subplot(spec[1, 1])\n",
    "    ax4 = fig.add_subplot(spec[2, 0])\n",
    "    ax5 = fig.add_subplot(spec[2, 1])\n",
    "    \n",
    "    dx_pred = x_pred - x_true[:-1]\n",
    "#     ax1.plot(dx_pred.mean(1).cpu().numpy(),color='red', label=\"Model\")\n",
    "    ax1.plot(x_pred[:,2].cpu().numpy(),color='red', label=\"Model\")\n",
    "    ax1.set_ylabel(\"dx/dt\")\n",
    "    ax1.set_xlabel(\"Time\")\n",
    "    ax1.set_title(\"Witheld test data\")\n",
    "    dx_true = x_true[1:] - x_true[:-1]\n",
    "    ax1.plot(dx_true.mean(1).cpu().numpy(),color=\"gray\",linewidth=5, alpha=0.7,label=\"Truth\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    if not A_true==None:\n",
    "        mymax = max(A_true.max(), A.max())\n",
    "        mymin = min(A_true.min(), A.min())\n",
    "        im = ax2.imshow(A_true.cpu(),vmin=mymin,vmax=mymax)\n",
    "        ax2.set_title(\"A true\")\n",
    "        ax3.imshow(A.cpu(),vmin=mymin,vmax=mymax)\n",
    "        ax3.set_title(\"A model\")\n",
    "    else:\n",
    "        ax3.imshow(A.cpu())\n",
    "#     cax,kw = mpl.colorbar.make_axes([ax2, ax3])\n",
    "#     fig.colorbar(im, cax=cax, **kw)\n",
    "    \n",
    "    if not B_true==None:\n",
    "        mymax = max(B_true.max(), B.max())\n",
    "        mymin = min(B_true.min(), B.min())\n",
    "        im2 = ax4.imshow(B_true.cpu(),vmin=mymin,vmax=mymax)\n",
    "        ax4.set_title(\"B true\")\n",
    "        ax5.imshow(B.cpu(),vmin=mymin,vmax=mymax)\n",
    "        ax5.set_title(\"B model\")\n",
    "    else:\n",
    "        ax5.imshow(B.cpu())\n",
    "#     cax,kw = mpl.colorbar.make_axes([ax4, ax5])\n",
    "#     fig.colorbar(im2, cax=cax, **kw)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, datetime\n",
    "LF_CODE_PATH = os.path.expanduser('~/projects/LFAnalyze/code')\n",
    "FT_CODE_PATH = os.path.expanduser('~/projects/fishTrax/code/analysis/')\n",
    "FD_CODE_PATH = os.path.expanduser('~/projects/fish_despair_notebooks/src/')\n",
    "sys.path.insert(0,LF_CODE_PATH)\n",
    "sys.path.insert(0,FT_CODE_PATH)\n",
    "sys.path.insert(0,FD_CODE_PATH)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pims\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import skimage.io\n",
    "import visualization_utils as vizutil\n",
    "import seaborn as sns\n",
    "from skimage.filters import gaussian_filter\n",
    "\n",
    "import passivity_2p_imaging_utils as p2putils\n",
    "reload(p2putils)\n",
    "tmp_dir = '/tmp/'\n",
    "all_data = p2putils.get_all_datasets(tmp_dir=tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = all_data['e'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first shock started at t=', f.get_shock_start_time(), 's')#shock start times\n",
    "print('The first slice of the frame began being imaged at t=', f.frame_st[0,0], 's') #time at which each slice was imaged [#samples X #slices]\n",
    "print('The first tail movement started at=', f.tail_movement_start_times[0], 's') #tail movement times - movements separted into forward swims, turns and escapes.\n",
    "print('Num z-planes imaged:', f.num_zplanes)\n",
    "print('Volume-Rate:', 1/np.diff(f.frame_st[:,0]).mean()) #frame_st is #frames x #slices, we examine interval between imaging first slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo code for examining movie of tail (slowed by factor of 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_ndx = 10\n",
    "clip = f.get_tail_movie_clip(f.tail_movement_start_times[movement_ndx]-.1, \n",
    "                             f.tail_movement_end_times[movement_ndx]+.1, \n",
    "                             playback_speed_factor=.2)\n",
    "clip.ipython_display(width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_window_starts = np.arange(-300,31*60,60) + f.get_shock_start_time() #1 minutes windows around start of shock\n",
    "rate_window_centers = rate_window_starts + 30\n",
    "rate_windows_adj = rate_window_starts - f.td_time[0] #corrected for the fact that get_movement_rate wants time relative to start of tail imaging.\n",
    "plt.plot(rate_window_centers - f.get_shock_start_time(), [f.get_movement_rate([x,x+60], bExcShockResp=False) for x in rate_windows_adj])\n",
    "plt.axvline(0,c='r')\n",
    "plt.ylabel('Movement Rate (Hz)')\n",
    "plt.xlabel('Time Relative to First Shock (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = f.get_roi_table() #this can be slow to run the first time as data is loaded from files\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = f.get_signals_raw(z=None)\n",
    "# Mf = hbutils.df_over_f(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also various methods for grabbing the raw imaging data:  \n",
    "get_tif_as_vol  \n",
    "get_tif_rasl\n",
    "\n",
    "We can use this to visualize a few ROIs in particular plane/slice and brain region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=10\n",
    "\n",
    "#Create a background image by averaging 200 frames and adjusting the gamma.\n",
    "back_img = np.power(f.get_tif_rasl_as_vol(z,range(1,200)).mean(axis=2),.4)\n",
    "\n",
    "#Select rois in raphe in this slices, and get their coordinates.\n",
    "coords = df[(df.z==z)].coords\n",
    "\n",
    "#Overlay the ROIs on the background image and display:\n",
    "img = vizutil.overlay_coords(back_img, coords, [0,0,1], alpha=.5)\n",
    "plt.figure(figsize=[20,20])\n",
    "plt.imshow(img,interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poly_area(x,y):\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def get_roi_area(poly_coords):\n",
    "    \"\"\"Get area of an rois.\n",
    "    Args:\n",
    "    poly_coords: is numpy matrix #vertices x 2.  NAN rows mean coords separates multiple polygons.\n",
    "    \"\"\"\n",
    "    #split poly_coords into list of polys based on nan rows\n",
    "    poly_list = np.split(poly_coords, np.where(np.isnan(poly_coords).any(axis=1))[0])\n",
    "    \n",
    "    #clean up by removing nan rows\n",
    "    poly_list = [poly[~np.isnan(poly).any(axis=1)] for poly in poly_list] #remove nan\n",
    "    \n",
    "    areas = [get_poly_area(poly[:,0],poly[:,1]) for poly in poly_list]\n",
    "    return sum(areas)\n",
    "\n",
    "def get_cnmf_rois_in_region(self, z, region_name):\n",
    "    #return bool array of length num rois in plane. true if in region.\n",
    "    centroids = self.get_roi_centroids(z)\n",
    "    region_polys = self.region_polys\n",
    "    poly = region_polys[region_name]['polys'][z]\n",
    "    if len(poly) < 3:\n",
    "        return np.zeros(centroids.shape[0],dtype=bool)\n",
    "    else:\n",
    "        return mpl.path.Path(poly).contains_points(centroids)\n",
    "    \n",
    "def get_cnmf_roi_table_and_signals(self, z=None, region=None):\n",
    "    #z indexed from 0\n",
    "    assert(z==None or region==None)\n",
    "    if not hasattr(self, '_df_cnmf_all_rois'):\n",
    "        region_polys = self.region_polys\n",
    "        self._df_cnmf_all_rois = pd.DataFrame()\n",
    "        self._cnmf_signals_all = []\n",
    "\n",
    "        for nz in xrange(self.num_zplanes):\n",
    "            #Load CNMF results for each plane\n",
    "            fn_cnmf = self.data_prefix+'_plane%d_cnmf_final.npz'%(nz+1)\n",
    "            d = np.load(fn_cnmf)\n",
    "            contours = d['contours']\n",
    "            centroids = np.array([roi['CoM'] for roi in contours])\n",
    "            #Not some ROIs are not contigous... are split in two or more parts.\n",
    "            #In this case there is an NaN row in the poly list to separate sections\n",
    "            #the poly area funciton handles this.\n",
    "\n",
    "            #Add to dataframe\n",
    "            df_rois = pd.DataFrame()\n",
    "            df_rois['fishid'] = [self.fishid]*len(contours)\n",
    "            df_rois['z'] = np.ones(len(contours), dtype=int)*nz\n",
    "            df_rois['zndx'] = np.arange(len(contours), dtype=int)\n",
    "            df_rois['poly'] = [contour['coordinates'] for contour in contours]\n",
    "            df_rois['centroid_x'] = centroids[:,0]\n",
    "            df_rois['centroid_y'] = centroids[:,1]\n",
    "            df_rois['area'] = [get_roi_area(contour['coordinates']) for contour in contours]\n",
    "            for region_name in region_polys:\n",
    "                region_poly = region_polys[region_name]['polys'][nz]\n",
    "                if len(region_poly) < 3:\n",
    "                    region_bndx = np.zeros(centroids.shape[0],dtype=bool)\n",
    "                else:\n",
    "                    region_bndx = mpl.path.Path(region_poly).contains_points(centroids)\n",
    "                df_rois['in_'+region_name] = region_bndx\n",
    "            self._df_cnmf_all_rois = self._df_cnmf_all_rois.append(df_rois, ignore_index=True)\n",
    "\n",
    "            self._cnmf_signals_all.append(d['C'])\n",
    "\n",
    "\n",
    "        #Convert boolean region columsn to a column containing region name\n",
    "        reg_columns = [item.startswith('in_') for item in self._df_cnmf_all_rois.columns]\n",
    "        temp = np.array(self._df_cnmf_all_rois.ix[:,reg_columns])\n",
    "        reg_ndx = np.array([np.where(row)[0][0] if len(np.where(row)[0])>0 \n",
    "                                                else np.nan for row in temp])\n",
    "        reg_names = np.array([name[3:] for name in self._df_cnmf_all_rois.columns[reg_columns]])\n",
    "        reg_names = np.hstack([reg_names,['other']])\n",
    "        reg_ndx[np.isnan(reg_ndx)] = len(reg_names)-1\n",
    "        self._df_cnmf_all_rois['region'] = reg_names[reg_ndx.astype(int)]\n",
    "\n",
    "        #Split region in a hemisphere and bilateral region name\n",
    "        self._df_cnmf_all_rois['hemisphere'] = [reg[:1] if (reg.startswith('l_') or reg.startswith('r_')) \n",
    "                                       else 'w' for reg in self._df_cnmf_all_rois['region']]\n",
    "        self._df_cnmf_all_rois['region_bilat'] = [reg[2:] if (reg.startswith('l_') or reg.startswith('r_')) \n",
    "                                               else reg for reg in self._df_cnmf_all_rois['region']]\n",
    "\n",
    "        #stack signals\n",
    "        self._cnmf_signals_all = np.vstack(self._cnmf_signals_all)\n",
    "\n",
    "    if z is not None:\n",
    "        bndx = self._df_cnmf_all_rois['z']==z\n",
    "    elif region is not None:\n",
    "        bndx = self._df_cnmf_all_rois['in_'+region]==True\n",
    "    else:\n",
    "        bndx = np.ones(self._df_cnmf_all_rois.shape[0], dtype=bool)\n",
    "\n",
    "    return self._df_cnmf_all_rois[bndx].copy(), self._cnmf_signals_all[bndx,:].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=9\n",
    "df, sig = f.get_cnmf_roi_table_and_signals()\n",
    "\n",
    "#Create a background image by averaging 200 frames and adjusting the gamma.\n",
    "back_img = np.power(f.get_tif_rasl_as_vol(z,range(1,200)).mean(axis=2),.4)\n",
    "\n",
    "#Select rois in raphe in this slices, and get their coordinates.\n",
    "poly_coords = df[(df.z==z)].poly\n",
    "poly_coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "\n",
    "#Overlay the ROIs on the background image and display:\n",
    "img = vizutil.overlay_coords(back_img, poly_coords, [0,0,1], alpha=.5)\n",
    "plt.figure(figsize=[20,20])\n",
    "plt.imshow(img,interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, sig = f.get_cnmf_roi_table_and_signals()\n",
    "# bndx = (df.in_r_MHb) & (df.z==4)\n",
    "# plt.plot(sig[bndx,:].mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr(x, y):\n",
    "    \"\"\"\n",
    "    Mimics `scipy.stats.pearsonr`\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : 1D torch.Tensor\n",
    "    y : 1D torch.Tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r_val : float\n",
    "        pearsonr correlation coefficient between x and y\n",
    "    \n",
    "    Scipy docs ref:\n",
    "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\n",
    "    \n",
    "    Scipy code ref:\n",
    "        https://github.com/scipy/scipy/blob/v0.19.0/scipy/stats/stats.py#L2975-L3033\n",
    "    Example:\n",
    "        >>> x = np.random.randn(100)\n",
    "        >>> y = np.random.randn(100)\n",
    "        >>> sp_corr = scipy.stats.pearsonr(x, y)[0]\n",
    "        >>> th_corr = pearsonr(torch.from_numpy(x), torch.from_numpy(y))\n",
    "        >>> np.allclose(sp_corr, th_corr)\n",
    "    \"\"\"\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "    xm = x.sub(mean_x)\n",
    "    ym = y.sub(mean_y)\n",
    "    r_num = xm.dot(ym)\n",
    "    r_den = torch.norm(xm, 2) * torch.norm(ym, 2)\n",
    "    r_val = r_num / r_den\n",
    "    return r_val\n",
    "\n",
    "def corrcoef(x):\n",
    "    \"\"\"\n",
    "    Mimics `np.corrcoef`\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : 2D torch.Tensor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    c : torch.Tensor\n",
    "        if x.size() = (5, 100), then return val will be of size (5,5)\n",
    "\n",
    "    Numpy docs ref:\n",
    "        https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html\n",
    "    Numpy code ref: \n",
    "        https://github.com/numpy/numpy/blob/v1.12.0/numpy/lib/function_base.py#L2933-L3013\n",
    "\n",
    "    Example:\n",
    "        >>> x = np.random.randn(5,120)\n",
    "        # result is a (5,5) matrix of correlations between rows\n",
    "        >>> np_corr = np.corrcoef(x)\n",
    "        >>> th_corr = corrcoef(torch.from_numpy(x))\n",
    "        >>> np.allclose(np_corr, th_corr.numpy())\n",
    "        # [out]: True\n",
    "    \"\"\"\n",
    "    # calculate covariance matrix of rows\n",
    "    mean_x = torch.mean(x, 1, keepdim=True)\n",
    "    xm = x.sub(mean_x.expand_as(x))\n",
    "    c = xm.mm(xm.t())\n",
    "    c = c / (x.size(1) - 1)\n",
    "\n",
    "    # normalize covariance matrix\n",
    "    d = torch.diag(c)\n",
    "    stddev = torch.pow(d, 0.5)\n",
    "    c = c.div(stddev.expand_as(c)+1e-8)\n",
    "    c = c.div(stddev.expand_as(c).t()+1e-8)\n",
    "\n",
    "    # clamp between -1 and 1\n",
    "    # probably not necessary but numpy does it\n",
    "    c = torch.clamp(c, -1.0, 1.0)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_corr_cnmf = corrcoef(T.from_numpy(sig).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_linkage = hierarchy.linkage(\n",
    "    distance.pdist(neuron_corr_cnmf), method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_linkage_clusters = hierarchy.fcluster(cnmf_linkage,4,criterion='maxclust')\n",
    "# includes all neurons, even those without cluster\n",
    "cluster_by_neuron = np.zeros(sig.shape[0]+1)\n",
    "for i,v in enumerate(cnmf_linkage_clusters):\n",
    "    cluster_by_neuron[i] = v\n",
    "cluster_by_neuron = cluster_by_neuron[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclust = int(cluster_by_neuron.max())\n",
    "plt.subplots(nclust,1, figsize=[10,5*nclust])\n",
    "for clust in range(1,nclust+1):\n",
    "    #Select rois in raphe in this slices, and get their coordinates.\n",
    "    poly_coords = df[(cluster_by_neuron==clust) & (df.z==z)].poly\n",
    "    poly_coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "    plt.subplot(nclust,1,clust)\n",
    "    #Overlay the ROIs on the background image and display:\n",
    "    img = vizutil.overlay_coords(back_img, poly_coords, [0,0,1], alpha=.5)\n",
    "    plt.imshow(img,interpolation='nearest')\n",
    "    plt.title(\"Cluster {}\".format(clust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a,0)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    rm = ret[n - 1:] / n\n",
    "    pad_start = np.full((n-1,rm.shape[1]), rm[0])\n",
    "    return np.vstack([pad_start, rm])\n",
    "\n",
    "def ewma(data,span):\n",
    "    df = DataFrame(data)\n",
    "    return df.ewm(span).mean().values\n",
    "\n",
    "def df_f(x,ma_window=6,span=6):\n",
    "    u = moving_average(x,ma_window)\n",
    "    return ewma((x - u)/u, span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_t = T.from_numpy(sig.T.astype(np.float32)).cuda()\n",
    "# M_T = T.from_numpy(M[bndx].T)\n",
    "\n",
    "# dff_norm = (sig_t - dff.mean(0))/(dff.std(0)+1e-8)\n",
    "# M_norm = (M_T - M_T.mean(0))/(M_T.std(0)+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = np.float32\n",
    "x_fish = F.normalize(sig_t,0)\n",
    "time_fish = T.from_numpy(f.frame_st.mean(1).astype(dtype)).cuda()\n",
    "if dtype==np.float16:\n",
    "    u_fish = T.cuda.HalfTensor(time_fish.shape).zero_()\n",
    "    p_fish = T.cuda.HalfTensor(time_fish.shape).zero_()\n",
    "else:\n",
    "    u_fish = T.cuda.FloatTensor(time_fish.shape).zero_()\n",
    "    p_fish = T.cuda.FloatTensor(time_fish.shape).zero_()\n",
    "u_fish[numpy.searchsorted(f.frame_et[:,-1], f.shock_st,side=\"left\")] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsCluster(nn.Module,):\n",
    "    def __init__(self, nfeatures, neuron_cluster_map, n_future_steps,dtype=T.float32, scale=1,std=0.1):\n",
    "        \"\"\"DCM model with dynamics between nclusters = max(neuron_cluster_map) features and dense mapping from\n",
    "        nfeatures -> nclusters\"\"\"\n",
    "        super(DynamicsCluster, self).__init__()\n",
    "        if dtype==T.float32:\n",
    "            tensor = T.cuda.FloatTensor\n",
    "        elif dtype==T.float16:\n",
    "            tensor = T.cuda.HalfTensor\n",
    "        nclusters = int(max(neuron_cluster_map))\n",
    "        self.nclusters = nclusters\n",
    "        self.cluster = []\n",
    "        for i in range(nclusters):\n",
    "            idx = np.argwhere(neuron_cluster_map==i+1)[:,0]\n",
    "            self.cluster.append(idx)\n",
    "        self.Dense1 = nn.Parameter(tensor(nfeatures).normal_(std), requires_grad=True)\n",
    "#         self.Dense2 = nn.Parameter(tensor(nfeatures).normal_(std), requires_grad=True)\n",
    "        self.A = nn.Parameter(tensor(nclusters,nclusters).normal_(std),requires_grad=True)\n",
    "        self.B = nn.Parameter(tensor(nclusters,nclusters).normal_(std),requires_grad=True)\n",
    "        self.C = nn.Parameter(tensor(nclusters).normal_(std),requires_grad=True)\n",
    "        self.n_future_steps = n_future_steps\n",
    "        self.tensor = tensor\n",
    "        self.scale = scale\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, u, p, x_true, n_future_steps=None):\n",
    "        if n_future_steps==None:\n",
    "            n_future_steps = self.n_future_steps\n",
    "        # batch x time x feature\n",
    "        x = self.tensor(x_true.shape[0], 1+x_true.shape[1], x_true.shape[2]).zero_()\n",
    "        x[:,0] = x_true[:,0]\n",
    "        for t in range(n_future_steps):\n",
    "            X = T.cuda.FloatTensor(x_true.shape[0],self.nclusters, 1).zero_()\n",
    "            for i in range(self.nclusters):\n",
    "                idx = self.cluster[i]\n",
    "                # broadcast inner product\n",
    "                X[:,i] = T.matmul(x_true[:,t,None,idx],self.Dense1[idx])\n",
    "            dxdt = (T.matmul((self.A + p[:,t,None,None]*self.B), X).squeeze()) + u[ :,t,None]*self.C\n",
    "            for i in range(self.nclusters):\n",
    "                idx = self.cluster[i]\n",
    "                x[:,t+1,idx] = dxdt[:,i,None]/self.Dense1[idx] + x[:,t,idx]\n",
    "        return x[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future_steps = 1\n",
    "batch_size = 64\n",
    "\n",
    "data = FishSeqData(u_fish,p_fish,x_fish,n_future_steps)\n",
    "model = DynamicsCluster(data.nfeatures,cluster_by_neuron,n_future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,data,5,(1e-4,0),(1e-4,0),(1e-4,0),(1e-4,0), lr=1e-2,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = FishSeqData(u_fish,p_fish,x_fish,1)\n",
    "pred = predict(model, data).squeeze()\n",
    "ncol, nrow = (5,3)\n",
    "fig, ax = plt.subplots(nrow, ncol, figsize=(19,10))\n",
    "\n",
    "# ax.set_ylabel(\"dx/dt\")\n",
    "# ax.set_xlabel(\"Time\")\n",
    "# ax.set_title(\"Training data\")\n",
    "dx_true = x_fish[1:] - x_fish[:-1]\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        n = np.random.randint(0,len(x_fish))\n",
    "        ax[i,j].plot(pred[:,n].cpu().numpy(),color='red', alpha=0.6,linewidth=1, label=\"Model\")\n",
    "        ax[i,j].plot(dx_true[:,n].cpu().numpy(),color=\"gray\", linewidth=1, alpha=0.6,label=\"Truth\")\n",
    "        ax[i,j].set_title(\"Neuron {}\".format(n))\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = FishSeqData(u_fish,p_fish,x_fish,1)\n",
    "pred = predict(model, data).squeeze()\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize=(10,10))\n",
    "\n",
    "# ax.set_ylabel(\"dx/dt\")\n",
    "# ax.set_xlabel(\"Time\")\n",
    "# ax.set_title(\"Training data\")\n",
    "dx_true = x_fish[1:] - x_fish[:-1]\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        n = np.random.randint(0,len(x_fish))\n",
    "        ax[i,j].plot(pred[:,n].cpu().numpy(),color='red', alpha=0.6,linewidth=1, label=\"Model\")\n",
    "        ax[i,j].plot(dx_true[:,n].cpu().numpy(),color=\"gray\", linewidth=1, alpha=0.6,label=\"Truth\")\n",
    "        ax[i,j].set_title(\"Neuron {}\".format(n))\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.A.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.A.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_regions = [u'in_r_cerebellum', u'in_l_cerebellum', u'in_l_vthal',\n",
    "       u'in_l_tectum', u'in_l_raphe', u'in_r_hind', u'in_l_hind',\n",
    "       u'in_l_dthal', u'in_r_tectum', u'in_r_LHb', u'in_r_dthal',\n",
    "       u'in_r_raphe', u'in_r_tel',\n",
    "       u'in_l_MHb', u'in_l_tel', u'in_r_MHb', u'in_l_LHb', u'in_r_vthal']\n",
    "\n",
    "regions = df.columns[np.where([c in ortho_regions for c in df.columns])]\n",
    "df = df.assign(region_id=pd.Series(np.full(len(df),-1).astype(np.int32)).values)\n",
    "for i, region in enumerate(regions):\n",
    "    idx = np.where(df[region])[0]\n",
    "    df.loc[idx, \"region_id\"] = i\n",
    "df[regions].sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_map[[20,21,22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros([4,4])\n",
    "x[[0,2],[0,2]] = 2\n",
    "df_test = DataFrame({\"region_id\":[0,1,0,1]})\n",
    "plot_matrix_by_region(x,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.from_numpy(x.astype(np.float32)).norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(row,regions):\n",
    "    return np.where(row[regions])[0][0]\n",
    "\n",
    "def plot_matrix_by_region(W, df):\n",
    "    neuron_map = np.argsort(df.where(df[\"region_id\"]>=0)[\"region_id\"]) # no region is -1\n",
    "    nHasRegion = np.sum(neuron_map!=-1) + 1 # last -1 index will be removed\n",
    "    newW = np.zeros(nHasRegion,nHasRegion)\n",
    "    new_order = neuron_map[np.arange(W.shape[0])]\n",
    "    new_order = new_order[new_order>=0]\n",
    "    newW = W[new_order]\n",
    "    newW = newW[:,new_order]\n",
    "    plt.imshow(newW,vmin=-1e-3,vmax=1e-3)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix_by_region(model.A.cpu().detach().numpy(),df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustergrid = sb.clustermap(model.A.cpu().detach().numpy(), method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustergrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"cnmf_f01555\",cnmf=sig.T, raw=M.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.A.()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.A.cpu().detach().numpy(),vmin=-0.005,vmax=0.005)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.eq(model.A,T.cuda.FloatTensor(model.A.shape).zero_()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.B[:100,:100].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.eq(model.B,T.cuda.FloatTensor(model.A.shape).zero_()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sort A by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.eq(model.A,T.cuda.FloatTensor(model.A.shape).zero_())[:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fish[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fish[0,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(u_fish[None,None,0],p_fish[None,None,0],x_fish[None,[0]],1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fish[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.B[:100,:100].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.save(model.state_dict(),\"fish_cnmf_decent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p27)",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
