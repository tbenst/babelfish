{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster.bicluster import SpectralBiclustering\n",
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "from sklearn.metrics import consensus_score\n",
    "\n",
    "import scipy.linalg\n",
    "from functools import partial\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sparse\n",
    "from scipy import stats\n",
    "from __future__ import print_function\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from pandas import DataFrame\n",
    "\n",
    "import os, sys, datetime\n",
    "LF_CODE_PATH = os.path.expanduser('~/projects/LFAnalyze/code')\n",
    "FT_CODE_PATH = os.path.expanduser('~/projects/fishTrax/code/analysis/')\n",
    "FD_CODE_PATH = os.path.expanduser('~/projects/fish_despair_notebooks/src/')\n",
    "sys.path.insert(0,LF_CODE_PATH)\n",
    "sys.path.insert(0,FT_CODE_PATH)\n",
    "sys.path.insert(0,FD_CODE_PATH)\n",
    "\n",
    "import passivity_2p_imaging_utils as p2putils\n",
    "import visualization_utils as vizutil\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "reload(p2putils)\n",
    "tmp_dir = '/tmp/'\n",
    "all_data = p2putils.get_all_datasets(tmp_dir=tmp_dir)\n",
    "\n",
    "fishIdx = [(\"e\", 2),  (\"e\", 5), (\"c\", 1),  (\"c\", 6),  (\"enp\", 1), (\"enp\", 5)]\n",
    "indicator = 0\n",
    "\n",
    "f = all_data[fishIdx[indicator][0]][fishIdx[indicator][1]]\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "# df, sig = f.get_cnmf_roi_table_and_signals()\n",
    "M = f.get_signals_raw(z=None)\n",
    "\n",
    "# data = np.load(\"../cnmf_f01555.npz\")\n",
    "# cnmf = data['cnmf'].astype(np.float32)\n",
    "# raw = data['raw'].astype(np.float32)\n",
    "# del data\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    # a is a signal\n",
    "    ret = np.cumsum(a,0) # sum over time\n",
    "    ret[n:] = ret[n:] - ret[:-n] # diff of n samples back\n",
    "    rm = ret[n - 1:] / n\n",
    "    pad_start = np.full((n-1,rm.shape[1]), rm[0])\n",
    "    return np.vstack([pad_start, rm])\n",
    "\n",
    "def ewma(data,span):\n",
    "    \"exponential weighted moving average.\"\n",
    "    df = DataFrame(data)\n",
    "    return df.ewm(span).mean().values\n",
    "\n",
    "def df_f(x,ma_window=6,span=6):\n",
    "    u = moving_average(x,ma_window)\n",
    "    return ewma((x - u)/u, span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neurons = sig.T\n",
    "neurons = M.T\n",
    "\n",
    "# neuron_ids = np.sort(np.argsort(neurons.std(0))[-5000:])\n",
    "df = f.get_roi_table()\n",
    "# df = df.iloc[neuron_ids]\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# neurons = neurons[:,neuron_ids]\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "neurons = signal.decimate(neurons,4, axis=0)\n",
    "\n",
    "neurons = df_f(neurons).astype(np.float32)\n",
    "neurons = (neurons - neurons.mean(0))/(neurons.std(0)+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_regions = [u'in_l_cerebellum', u'in_r_cerebellum', \n",
    "    u'in_l_vthal', u'in_r_vthal', u'in_l_tectum', u'in_r_tectum', \n",
    "    u'in_l_raphe', u'in_r_raphe', u'in_l_hind', u'in_r_hind', \n",
    "    u'in_l_dthal', u'in_r_dthal', u'in_l_LHb', u'in_r_LHb', \n",
    "    u'in_l_tel', u'in_r_tel', u'in_l_MHb',  u'in_r_MHb']\n",
    "\n",
    "regions = []\n",
    "for r in ortho_regions:\n",
    "    if r in df.columns:\n",
    "        regions.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_pca = {}\n",
    "for r in regions:\n",
    "    X = neurons[:,df[r]]\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(X)\n",
    "    transform = pca.fit_transform(X)\n",
    "    region_pca[r] = {\"components\": pca.components_, \"explained_var\": pca.explained_variance_ratio_, \"transform\": transform}\n",
    "neurons_pca_transform = np.concatenate([region_pca[r]['transform'] for r in regions],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pairwise_batches(X, lag=1, startIdx=None, endIdx=None, bias=True):\n",
    "    \"\"\"constructs batch x nT-lag x nBeta. i*ncol+j index is granger model of influence from i->j.\n",
    "    \n",
    "    Use startIdx and endIdx to choose a certain number of rows,\n",
    "    eg startIdx=0, endIdx=5, X.shape[1]=100 would be a batch of 100^2=10,000.\n",
    "    \n",
    "    WARNING: if endIdx>N, will fill with 1s so batch stays same size.\"\"\"\n",
    "    \n",
    "    nT, N = X.shape\n",
    "    if endIdx:\n",
    "        bz = (endIdx - startIdx) * N\n",
    "    else:\n",
    "        startIdx=0\n",
    "        endIdx=N\n",
    "        bz = N**2\n",
    "#     print([bz, nT-lag, 2*lag+bias])\n",
    "    granger = np.ones([bz, nT-lag, 2*lag+bias])\n",
    "    null = np.ones([bz, nT-lag, lag+bias])\n",
    "    newY = np.ones([bz, nT-lag, 1])\n",
    "    Y = X[lag:]\n",
    "    # time x neuron x lag\n",
    "    l = 0\n",
    "    X_lag = np.concatenate([X[l:-(lag-l)][:,:,None] for l in reversed(range(lag))], 2)\n",
    "    biasTerm = np.ones([nT-lag,1])\n",
    "    for i, n1 in enumerate(range(startIdx,min(N,endIdx))):\n",
    "        for j, n2 in enumerate(range(N)):\n",
    "            if bias:\n",
    "#                 print([x.shape for x in [X_lag[:,n1], X_lag[:,n2], biasTerm]])\n",
    "                grow = np.concatenate([X_lag[:,n1], X_lag[:,n2], biasTerm], 1)\n",
    "                nrow = np.concatenate([X_lag[:,n2], biasTerm], 1)\n",
    "            else:\n",
    "                grow = np.concatenate([X_lag[:,n1], X_lag[:,n2]], 1)\n",
    "                nrow = np.concatenate([X_lag[:,n2]], 1)\n",
    "#             print(i,j)\n",
    "            granger[i*N+j] = grow\n",
    "            null[i*N+j] = nrow\n",
    "            newY[i*N+j] = Y[:,[n2]]\n",
    "    return newY, granger, null\n",
    "\n",
    "def f_value(resA, resB, n_params_A, n_params_B, n):\n",
    "    return ( (resA-resB)/(n_params_B-n_params_A) ) / (resB/(n-n_params_B))\n",
    "\n",
    "class Granger():\n",
    "    def __init__(self, x_granger_shape, x_null_shape, y_shape, sess, l2=0.0):\n",
    "        self.x_granger = tf.placeholder(dtype=tf.float32, shape=x_granger_shape)\n",
    "        self.x_null = tf.placeholder(dtype=tf.float32, shape=x_null_shape)\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=y_shape)\n",
    "        self.n_params_granger = x_granger_shape[-1]\n",
    "        self.n_params_null = x_null_shape[-1]\n",
    "        self.l2 = l2\n",
    "        self.sess = sess\n",
    "        \n",
    "    def granger(self, X, Y):\n",
    "        gbeta = tf.matrix_solve_ls(self.x_granger, self.y, l2_regularizer=self.l2, fast=False)\n",
    "        g_residuals = tf.reduce_sum(((tf.matmul(self.x_granger,gbeta) - self.y)**2)[:,:,0], 1)\n",
    "        gbeta, g_residuals = self.sess.run([gbeta, g_residuals],\n",
    "                                      feed_dict={self.x_granger: X, self.y: Y})\n",
    "        return gbeta, g_residuals\n",
    "    \n",
    "    def null(self, X, Y):\n",
    "        gbeta = tf.matrix_solve_ls(self.x_null, self.y, l2_regularizer=self.l2, fast=False)\n",
    "        g_residuals = tf.reduce_sum(((tf.matmul(self.x_null,gbeta) - self.y)**2)[:,:,0], 1)\n",
    "        gbeta, g_residuals = self.sess.run([gbeta, g_residuals],\n",
    "                                      feed_dict={self.x_null: X, self.y: Y})\n",
    "        return gbeta, g_residuals\n",
    "\n",
    "def pairwise_granger_f_val(neurons, lag=1, bz=10):\n",
    "    \"all_gbeta is granger then autocorr then bias.\"\n",
    "    nT, N = neurons.shape\n",
    "    Y, x_granger, x_null = construct_pairwise_batches(neurons,lag,0,bz)\n",
    "    fvals = np.zeros([N,N])\n",
    "    all_gbeta = np.zeros([N,N,2*lag+1])\n",
    "    all_nbeta = np.zeros([N,N,lag+1])\n",
    "    with tf.Session() as sess:\n",
    "        granger = Granger(x_granger.shape, x_null.shape, Y.shape, sess=sess, l2=0.0)\n",
    "        for start in tqdm(range(0,N,bz)):\n",
    "            end = start+bz\n",
    "            Y, x_granger, x_null = construct_pairwise_batches(neurons,1,start,end)\n",
    "            gbeta, g_residuals = granger.granger(x_granger, Y)\n",
    "            nbeta, n_residuals = granger.null(x_null, Y)\n",
    "            batch_fvals = f_value(g_residuals, n_residuals, granger.n_params_granger, granger.n_params_null, N)\n",
    "            # account for N % batch != 0\n",
    "            true_end = min(start+bz,N)\n",
    "            batch_fvals = batch_fvals.reshape(-1,N)\n",
    "            batch_gbeta = gbeta.reshape(-1,N,2*lag+1)\n",
    "            batch_nbeta = nbeta.reshape(-1,N,lag+1)\n",
    "            if start+bz>N:\n",
    "                batch_true_end = N-start\n",
    "                batch_fvals = batch_fvals[:batch_true_end]\n",
    "                batch_gbeta = batch_gbeta[:batch_true_end]\n",
    "                batch_nbeta = batch_nbeta[:batch_true_end]\n",
    "            fvals[start:true_end] = batch_fvals\n",
    "            all_gbeta[start:true_end] = batch_gbeta\n",
    "            all_nbeta[start:true_end] = batch_nbeta\n",
    "    return fvals, all_gbeta, all_nbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvals, gbeta, nbeta = pairwise_granger_f_val(neurons_pca_transform, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = list(map(lambda x: x[3:],regions))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(fvals,vmin=0,vmax=3.5)\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(reg)), reg)\n",
    "plt.yticks(np.arange(len(reg)), reg)\n",
    "plt.title(\"{} F-value for granger causality on first PCA\".format(f.fishid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive\n",
    "mostly CCA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = neurons_pca_transform[:-1]\n",
    "Y = neurons_pca_transform[1:]\n",
    "n_components = 10\n",
    "cca = CCA(n_components=n_components)\n",
    "cca.fit(X,Y)\n",
    "CCA(copy=True, max_iter=500, n_components=1, scale=True, tol=1e-06)\n",
    "# cca.x_weights_\n",
    "# cca.y_weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.imshow(cca.x_weights_)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(cca.y_weights_)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(cca.y_weights_-cca.x_weights_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = list(map(lambda x: x[3:],regions))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(fvals)\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(reg)), reg)\n",
    "plt.yticks(np.arange(len(reg)), reg)\n",
    "plt.title(\"{} F-value for granger causality on first PCA\".format(f.fishid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = np.percentile(cca.x_weights_,99)\n",
    "vmin = np.percentile(cca.x_weights_,1)\n",
    "# vmean = np.mean(cca.x_weights_)\n",
    "cmap_ = cm.bwr\n",
    "# mpl.colors.Normalize(vmin=-1.,vmax=1.)\n",
    "norm_ = Normalize(vmin=vmin, vmax=vmax)\n",
    "m = cm.ScalarMappable(norm=norm_, cmap=cmap_)\n",
    "cmap = lambda x: m.to_rgba(x)[:3]\n",
    "p = np.array([map(cmap, cca.x_weights_[:100,i]) for i in range(n_components)])\n",
    "plt.imshow(p, vmin=0, vmax=1)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = [0,2,4,6,8,10]\n",
    "# Z = [4]\n",
    "nZ = len(Z)\n",
    "back_img = []\n",
    "for z in Z:\n",
    "    back_img.append(np.power(f.get_tif_rasl_as_vol(z,range(1,200)).mean(axis=2),.4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_color_coords(img, roi_coords, colors, alpha):\n",
    "    \"\"\"\n",
    "    Create an new image by overlaying per ROI coloring (each a set of coordinates) on backgroung image.\n",
    "    Note, this function currently rescales the background img to use the entire grayscale range (change this).\n",
    "    \n",
    "    img - a grayscale (single channel) image.\n",
    "    roi_coords - a list of N  (Mx2) arrays specifing sets of coords for each roi (N rois)\n",
    "    colors - a list of N rgb colors, e.g red is 1,0,0.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    over_img = (img - img.min()) / float(img.max() - img.min())\n",
    "    over_img = np.dstack((over_img, over_img, over_img))\n",
    "\n",
    "    if len(roi_coords) == 0:\n",
    "        return over_img\n",
    "    if (len(np.array(colors).shape)==1) and (np.array(colors).shape[0] == 3):\n",
    "        print('multi')\n",
    "        colors = np.array([colors]*len(roi_coords))\n",
    "    assert colors.shape[0] == len(roi_coords)\n",
    "\n",
    "    for coords, color in zip(roi_coords, colors):\n",
    "        over_img[coords[:,0],coords[:,1],:]*=(1-alpha)\n",
    "        over_img[coords[:,0],coords[:,1],:]+=color*alpha\n",
    "        #for x,y in coords:\n",
    "        #    over_img[x,y,:] = over_img[x,y,:]*(1-alpha)+color*alpha\n",
    "        #skimage.draw.set_color(img = over_img, \n",
    "        #                       coords = (coords[:,0],coords[:,1]),\n",
    "        #                       color = color)   \n",
    "    return over_img\n",
    "\n",
    "# nrows = len(row_clust_to_plot)\n",
    "# nZ = 1\n",
    "nrows = n_components\n",
    "plt.subplots(nrows,nZ, figsize=[8*nZ,4*nrows])\n",
    "for ic, clust in enumerate(range(1, n_components+1)):\n",
    "# for ic,clust in enumerate(row_clust_to_plot):\n",
    "    for iz, z in enumerate(Z):\n",
    "        idx = df.z==z\n",
    "        #Select rois in raphe in this slices, and get their coordinates.\n",
    "        coords = df[idx].coords\n",
    "        my_colors = np.array(map(cmap, cca.x_weights_[idx,ic]))\n",
    "#         poly_coords = df[(row_clusters==clust) & (df.z==z)].poly\n",
    "#         poly_coords = df[(row_clusters==clust)].poly\n",
    "#         coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "        plt.subplot(nrows,nZ,ic*nZ+iz+1)\n",
    "        #Overlay the ROIs on the background image and display:\n",
    "        # hack iz hardcode\n",
    "        img = overlay_color_coords(back_img[iz], coords, my_colors, alpha=0.5)\n",
    "        plt.imshow(img,interpolation='nearest')\n",
    "        plt.title(\"t-1 cluster {}, z={}\".format(clust,z),fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = len(row_clust_to_plot)\n",
    "# nZ = 1\n",
    "nrows = n_components\n",
    "plt.subplots(nrows,nZ, figsize=[8*nZ,4*nrows])\n",
    "for ic, clust in enumerate(range(1, n_components+1)):\n",
    "# for ic,clust in enumerate(row_clust_to_plot):\n",
    "    for iz, z in enumerate(Z):\n",
    "        idx = df.z==z\n",
    "        #Select rois in raphe in this slices, and get their coordinates.\n",
    "        coords = df[idx].coords\n",
    "        my_colors = np.array(map(cmap, cca.y_weights_[idx,ic]))\n",
    "#         poly_coords = df[(row_clusters==clust) & (df.z==z)].poly\n",
    "#         poly_coords = df[(row_clusters==clust)].poly\n",
    "#         coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "        plt.subplot(nrows,nZ,ic*nZ+iz+1)\n",
    "        #Overlay the ROIs on the background image and display:\n",
    "        # hack iz hardcode\n",
    "        img = overlay_color_coords(back_img[iz], coords, my_colors, alpha=0.5)\n",
    "        plt.imshow(img,interpolation='nearest')\n",
    "        plt.title(\"t-1 cluster {}, z={}\".format(clust,z),fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = len(col_clust_to_plot)\n",
    "plt.subplots(nrows,nZ, figsize=[8*nZ,4*nrows])\n",
    "for ic,clust in enumerate(range(1,g_nclust+1)):\n",
    "# for ic,clust in enumerate(col_clust_to_plot):\n",
    "    for iz, z in enumerate(Z):\n",
    "        #Select rois in raphe in this slices, and get their coordinates.\n",
    "        coords = df[(col_clusters==clust) & (df.z==z)].coords\n",
    "#         poly_coords = df[(row_clusters==clust) & (df.z==z)].poly\n",
    "#         coords = df[(col_clusters==clust)].coords\n",
    "#         poly_coords = df[(col_clusters==clust)].poly\n",
    "#         coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "        plt.subplot(nrows,nZ,ic*nZ+iz+1)\n",
    "        #Overlay the ROIs on the background image and display:\n",
    "        img = vizutil.overlay_coords(back_img[iz], coords, list(cm_cycle(clust)[:3]), alpha=1)\n",
    "        plt.imshow(img,interpolation='nearest')\n",
    "        plt.title(\"Col cluster {}, z={}\".format(clust,z),fontsize=18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
