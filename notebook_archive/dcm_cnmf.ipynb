{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import scipy.linalg\n",
    "from functools import partial\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "plt.ioff()\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sparse\n",
    "from scipy import stats\n",
    "from __future__ import print_function\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{t+1} = (A + pB)x_t + Cu_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passivity(t,k=0.05,t0=400):\n",
    "    return 1/(1+np.exp(-k*(t-t0)))\n",
    "\n",
    "def diag_idx(n):\n",
    "    return [i for i in range(n)],[i for i in range(n)]\n",
    "\n",
    "def makeStableMatrix(n, density=0.1):\n",
    "    # TODO: implement on GPU?\n",
    "    eig = np.diag(np.random.uniform(-1,0,n))\n",
    "    v = scipy.linalg.orth(np.random.uniform(-1,1,(n,n)))\n",
    "    A = np.matmul(np.matmul(v.T, eig), v)\n",
    "    # need to make A sparse\n",
    "    indices = np.reshape(np.indices((n,n)),[2,-1])\n",
    "    mask = np.eye(n)\n",
    "    # may create density slightly less than desired if choice is on diagonal\n",
    "    idx = indices[:,np.random.choice(np.arange(indices.shape[1]),int(np.ceil(density*n**2-n)),replace=False)]\n",
    "    mask[idx[0], idx[1]] = 1\n",
    "    A[np.logical_not(mask)] = 0\n",
    "    return A\n",
    "\n",
    "def step(A,B,C,u,p,x,dt=1):\n",
    "    dxdt = T.matmul((A + p*B), x) + C * u\n",
    "    return dxdt + x\n",
    "\n",
    "def step_batch(A,B,C,u,p,x,dt=1):\n",
    "    dxdt = (T.matmul((A + p[:,None,None]*B), x[:,:,None])).squeeze() + u[:,None]*C\n",
    "    return dxdt + x\n",
    "\n",
    "def sim(A,B,C,u,p,x0,e=0.05):\n",
    "    ntime = u.shape[0]\n",
    "    nfeatures = x0.shape[0]\n",
    "    time = T.arange(ntime,dtype=T.int32)\n",
    "    x = T.zeros(ntime, x0.shape[0]).cuda()\n",
    "    z = T.zeros(nfeatures).cuda()\n",
    "    x[0] = x0\n",
    "    for t in range(1,ntime):\n",
    "        if e==0:\n",
    "            x[t] = step(A,B,C,u[t],p[t],x[t-1])\n",
    "        else:\n",
    "            x[t] = step(A,B,C,u[t],p[t],x[t-1]) + T.normal(z,e).cuda()\n",
    "    return x\n",
    "\n",
    "def generate_data(sim_time, nstim,nfeatures, burn_in=400, test_time=200, e=0.01, k=0.05, t0=400, density=0.2):\n",
    "    ntime = sim_time+burn_in+test_time\n",
    "    time = T.from_numpy(np.arange(ntime)).cuda()\n",
    "    A_true = T.from_numpy(makeStableMatrix(nfeatures,density).astype(np.float32)).cuda()\n",
    "    B_true = T.from_numpy(makeStableMatrix(nfeatures,density).astype(np.float32)).cuda()\n",
    "    C_true = T.from_numpy(np.random.normal(0,0.05,nfeatures).astype(np.float32)).cuda()\n",
    "    # C_true = T.from_numpy(makeStableMatrix(nfeatures).astype(np.float32)).cuda()\n",
    "    x0 = T.from_numpy(np.random.normal(0,0.05,nfeatures).astype(np.float32)).cuda()\n",
    "    u = T.zeros(ntime).cuda()\n",
    "    u[np.arange(4,ntime,ntime/nstim)] = 1\n",
    "\n",
    "    p1 = np.vectorize(partial(passivity,k=k,t0=t0))\n",
    "    time = np.arange(ntime)\n",
    "    p_true = p1(time).astype(np.float32)\n",
    "    p_true = T.from_numpy(p_true).cuda()\n",
    "#     p_true = T.zeros(ntime).cuda() # TODO remove me\n",
    "    x_true = sim(A_true,B_true,C_true,u,p_true,x0,e)\n",
    "    train = (time[burn_in:-test_time], u[burn_in:-test_time],\n",
    "             p_true[burn_in:-test_time], x_true[burn_in:-test_time])\n",
    "    test = (time[-test_time:], u[-test_time:], p_true[-test_time:], x_true[-test_time:])\n",
    "    return A_true, B_true, C_true, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FishSeqData(Dataset):    \n",
    "    def __init__(self, u, p, x,n_future_steps=1):\n",
    "        self.x = nn.Parameter(x,requires_grad=False)\n",
    "        self.p = nn.Parameter(p,requires_grad=False)\n",
    "        self.u = nn.Parameter(u,requires_grad=False)\n",
    "        self.nfeatures = x.shape[1]\n",
    "        self.n_future_steps = n_future_steps\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)-self.n_future_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = slice(idx,idx+self.n_future_steps)\n",
    "        x_true_indices = slice(idx+1,idx+self.n_future_steps+1)\n",
    "        return (self.u[indices], self.p[indices],\n",
    "                self.x[indices], self.x[x_true_indices])\n",
    "\n",
    "class DynamicsSeq(nn.Module):\n",
    "    def __init__(self, nfeatures, n_future_steps,dtype=T.float32, scale=1,std=0.1):\n",
    "        super(DynamicsSeq, self).__init__()\n",
    "        if dtype==T.float32:\n",
    "            tensor = T.cuda.FloatTensor\n",
    "        elif dtype==T.float16:\n",
    "            tensor = T.cuda.HalfTensor\n",
    "                \n",
    "        self.A = nn.Parameter(tensor(nfeatures,nfeatures).normal_(std),requires_grad=True)\n",
    "        self.B = nn.Parameter(tensor(nfeatures,nfeatures).normal_(std),requires_grad=True)\n",
    "        self.C = nn.Parameter(tensor(nfeatures).normal_(std),requires_grad=True)\n",
    "        self.n_future_steps = n_future_steps\n",
    "        self.tensor = tensor\n",
    "        self.scale = scale\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, u, p, x_true, n_future_steps=None):\n",
    "        if n_future_steps==None:\n",
    "            n_future_steps = self.n_future_steps\n",
    "        x = self.tensor(x_true.shape[0], 1+x_true.shape[1], *x_true.shape[2:]).zero_()\n",
    "        x[:,0] = x_true[:,0]\n",
    "        for t in range(n_future_steps):\n",
    "            u[ :,t,None]*self.C\n",
    "            dxdt = (T.matmul((self.A + p[:,t,None,None]*self.B), x_true[:,t,:,None]).squeeze()) + u[ :,t,None]*self.C\n",
    "            x[:,t+1] = F.tanhshrink(dxdt) + x[:,t]\n",
    "        return x[:,1:]\n",
    "    \n",
    "#     def predict(self, u, p, x_init, n_future_steps=1):\n",
    "#         with T.no_grad():\n",
    "#             x = self.tensor(1+u.shape[0], x_init.shape[0]).zero_()\n",
    "#             x[0] = x_init\n",
    "#             for t in range(n_future_steps):\n",
    "#                 u[t,None]*self.C\n",
    "#                 dxdt = (T.matmul((self.A + p[t,None,None]*self.B), x[t,:,None]).squeeze()) + u[t,None]*self.C\n",
    "#                 x[t+1] = dxdt + x[t]\n",
    "#         return x[1:]\n",
    "\n",
    "    \n",
    "    def predict(self, u, p, x_init, n_future_steps=1):\n",
    "        with T.no_grad():\n",
    "            x = self.tensor(1+u.shape[0], x_init.shape[0]).zero_()\n",
    "            x[0] = x_init\n",
    "            for t in range(n_future_steps):\n",
    "                x[t+1] = self(u[:,[t]],p[:,[t]],x[None,[t]],1)\n",
    "        return x[1:]\n",
    "        \n",
    "class Dynamics(nn.Module):\n",
    "    def __init__(self, nfeatures):\n",
    "        super(Dynamics, self).__init__()\n",
    "        self.A = nn.Parameter(T.normal(T.zeros(nfeatures,nfeatures),0.5),requires_grad=True)\n",
    "        self.B = nn.Parameter(T.normal(T.zeros(nfeatures,nfeatures),0.5),requires_grad=True)\n",
    "        self.C = nn.Parameter(T.normal(T.zeros(nfeatures),0.5),requires_grad=True)\n",
    "\n",
    "    def forward(self, u, p, x):\n",
    "        return (x[:,0] + (T.matmul((self.A + p[:,0,None,None]*self.B), x[:,0,:,None]).squeeze()) + u[:,0,None] * self.C)[:,None]\n",
    "\n",
    "\n",
    "def train(model,data,nepochs=10, lambdaA=(1e-8, 1e-6), lambdaB=(1e-6, 1e-6), lambdaC=(1e-5, 1e-5), lr=0.1, verbose=True):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    if verbose:\n",
    "        A_loss = F.mse_loss(model.A.data,A_true)\n",
    "        print(\"A_loss: {}\".format(A_loss))\n",
    "    cum_mse_loss = 0\n",
    "    with T.no_grad():\n",
    "        for batch_data in tqdm(dataloader):\n",
    "            U,P,X, X_true = batch_data\n",
    "            X_pred = model(U,P,X)\n",
    "            cum_mse_loss += F.mse_loss(X_pred.float(),X_true.float())\n",
    "    print(\"mse_loss: {:3E}\".format(cum_mse_loss/len(dataloader)))\n",
    "#     optimizer = T.optim.SGD(model.parameters(),lr=lr)\n",
    "    optimizer = T.optim.Adam(model.parameters(),lr=lr,amsgrad=True)\n",
    "    for e in range(nepochs):\n",
    "        if verbose:\n",
    "            print(\"epoch {}: \".format(e), end=\"\")\n",
    "        cum_loss = 0\n",
    "        cum_mse_loss = 0\n",
    "        for batch_data in tqdm(dataloader):\n",
    "            U,P,X, X_true = batch_data\n",
    "            X_pred = model(U,P,X)\n",
    "            l_A = lambdaA[0] * model.A.norm(1) + lambdaA[1] * model.A.norm(2)\n",
    "            l_B = lambdaB[0] * model.B.norm(1) + lambdaB[1] * model.B.norm(2)\n",
    "            l_C = lambdaC[0] * model.C.norm(1) + lambdaC[1] * model.C.norm(2)\n",
    "            mse_loss = F.mse_loss(X_pred,X_true)\n",
    "            loss = mse_loss + l_A + l_B + l_C\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cum_loss += float(loss)\n",
    "            cum_mse_loss += float(mse_loss)\n",
    "            del X_pred, U,P,X, X_true, mse_loss, l_A, l_B, loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if verbose:\n",
    "            A_loss = F.mse_loss(model.A.data,A_true)\n",
    "            B_loss = F.mse_loss(model.B.data,B_true)\n",
    "            C_loss = F.mse_loss(model.C.data,C_true)\n",
    "            print(\"pred_loss: {}, A_loss: {}, B_loss: {}, C_loss: {}\".format(cum_loss,A_loss,B_loss,C_loss))\n",
    "        print(\"cum_loss: {:3E}, mse_loss: {:3E}\".format(cum_loss,cum_mse_loss/len(dataloader)))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"pred_loss: {}, A_loss: {}, B_loss: {}, C_loss: {}\".format(cum_loss,A_loss,B_loss,C_loss))\n",
    "        \n",
    "\n",
    "        \n",
    "def set_grad(params, params_with_grad):\n",
    "\n",
    "    for param, param_w_grad in zip(params, params_with_grad):\n",
    "        if param.grad is None:\n",
    "            param.grad = torch.nn.Parameter(param.data.new().resize_(*param.data.size()))\n",
    "        param.grad.data.copy_(param_w_grad.grad.data)\n",
    "    \n",
    "def train_fp16(model,data,nepochs=10, lambdaA=(1e-8, 1e-6), lambdaB=(1e-6, 1e-6), lr=0.1, verbose=True,loss_scale=1):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    param_copy = [param.clone().type(torch.cuda.FloatTensor).detach() for param in model.parameters()]\n",
    "    for batch_data in tqdm(dataloader):\n",
    "            U,P,X, X_true = batch_data\n",
    "            X_pred = model(U,P,X)\n",
    "            mse_loss = F.mse_loss(X_pred.float(),X_true.float())\n",
    "            print(\"pred loss: {}\".format(mse_loss))\n",
    "            break\n",
    "    \n",
    "    if verbose:\n",
    "        A_loss = F.mse_loss(param_copy[0].data/model.scale,A_true)\n",
    "        print(\"A_loss: {}\".format(A_loss))\n",
    "#         optimizer = T.optim.SGD(model.parameters(),lr=lr)\n",
    "    for param in param_copy:\n",
    "        param.requires_grad = True\n",
    "    optimizer = torch.optim.SGD(param_copy, lr)\n",
    "#     optimizer = T.optim.Adam(param_copy,lr=lr)\n",
    "    nfeatures = model.A.shape[0]\n",
    "#     nondiagonal = ~T.eye(nfeatures,dtype=T.uint8)\n",
    "    for e in range(nepochs):\n",
    "        if verbose:\n",
    "            print(\"epoch {}: \".format(e), end=\"\")\n",
    "        cum_loss = 0\n",
    "        for batch_data in tqdm(dataloader):\n",
    "            U,P,X, X_true = batch_data\n",
    "            X_pred = model(U,P,X)\n",
    "            l_A = lambdaA[0] * model.A.float().norm(1) + lambdaA[1] * model.A.float().norm(2)\n",
    "            l_B = lambdaB[0] * model.B.float().norm(1) + lambdaB[1] * model.B.float().norm(2)\n",
    "#             l_A = lambdaA[0] * model.A[nondiagonal].float().norm(1) + lambdaA[1] * model.A[nondiagonal].float().norm(2)\n",
    "#             l_B = lambdaB[0] * model.B.float().norm(1) + lambdaB[1] * model.B.float().norm(2)\n",
    "\n",
    "            mse_loss = F.mse_loss(X_pred,X_true).float()\n",
    "            loss = (mse_loss + l_A + l_B) * loss_scale\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            set_grad(param_copy, list(model.parameters()))\n",
    "            if loss_scale != 1:\n",
    "                for param in param_copy:\n",
    "                    param.grad.data = param.grad.data/loss_scale\n",
    "            optimizer.step()\n",
    "            params = list(model.parameters())\n",
    "            for i in range(len(params)):\n",
    "                params[i].data.copy_(param_copy[i].data)\n",
    "\n",
    "            cum_loss += float(loss)/loss_scale\n",
    "    #             del X_pred, U,P,X, X_true, mse_loss, l_A, l_B, loss\n",
    "    #             gc.collect()\n",
    "    #             torch.cuda.empty_cache()\n",
    "\n",
    "        if verbose:\n",
    "            A_loss = F.mse_loss(param_copy[0].data/model.scale,A_true)\n",
    "            B_loss = F.mse_loss(param_copy[1].data/model.scale,B_true)\n",
    "            C_loss = F.mse_loss(param_copy[2].data/model.scale,C_true)\n",
    "            print(\"pred_loss: {}, A_loss: {}, B_loss: {}, C_loss: {}\".format(cum_loss,A_loss,B_loss,C_loss))\n",
    "        print(\"pred_loss: {}\".format(cum_loss))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"pred_loss: {}, A_loss: {}, B_loss: {}, C_loss: {}\".format(cum_loss,A_loss,B_loss,C_loss))\n",
    "\n",
    "def predict(model,data, batch_size=32):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    with T.no_grad():\n",
    "        pred = T.cuda.FloatTensor(len(data),*data[0][2].shape).zero_()\n",
    "        for i, batch_data in enumerate(tqdm(dataloader)):\n",
    "            U,P,X, X_true = batch_data\n",
    "            X_pred = model(U,P,X)\n",
    "            pred[i*batch_size:(i+1)*batch_size] = X_pred\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "def model_v_truth(model,u,p,x_true,A_true=None, B_true=None):\n",
    "    if model.dtype==T.float16:\n",
    "        A = model.A.data.float()*model.scale\n",
    "        B = model.B.data.float()*model.scale\n",
    "    else:\n",
    "        A = model.A.data\n",
    "        B = model.B.data\n",
    "    x_pred = predict(model,data).squeeze()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    spec = gridspec.GridSpec(ncols=2, nrows=3)\n",
    "    anno_opts = dict(xy=(0.5, 0.5), xycoords='axes fraction',\n",
    "                     va='center', ha='center')\n",
    "\n",
    "    ax1 = fig.add_subplot(spec[0,0:])\n",
    "    ax2 = fig.add_subplot(spec[1, 0])\n",
    "    ax3 = fig.add_subplot(spec[1, 1])\n",
    "    ax4 = fig.add_subplot(spec[2, 0])\n",
    "    ax5 = fig.add_subplot(spec[2, 1])\n",
    "    \n",
    "    dx_pred = x_pred - x_true[:-1]\n",
    "#     ax1.plot(dx_pred.mean(1).cpu().numpy(),color='red', label=\"Model\")\n",
    "    ax1.plot(x_pred[:,2].cpu().numpy(),color='red', label=\"Model\")\n",
    "    ax1.set_ylabel(\"dx/dt\")\n",
    "    ax1.set_xlabel(\"Time\")\n",
    "    ax1.set_title(\"Witheld test data\")\n",
    "    dx_true = x_true[1:] - x_true[:-1]\n",
    "    ax1.plot(dx_true.mean(1).cpu().numpy(),color=\"gray\",linewidth=5, alpha=0.7,label=\"Truth\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    if not A_true==None:\n",
    "        mymax = max(A_true.max(), A.max())\n",
    "        mymin = min(A_true.min(), A.min())\n",
    "        im = ax2.imshow(A_true.cpu(),vmin=mymin,vmax=mymax)\n",
    "        ax2.set_title(\"A true\")\n",
    "        ax3.imshow(A.cpu(),vmin=mymin,vmax=mymax)\n",
    "        ax3.set_title(\"A model\")\n",
    "    else:\n",
    "        ax3.imshow(A.cpu())\n",
    "#     cax,kw = mpl.colorbar.make_axes([ax2, ax3])\n",
    "#     fig.colorbar(im, cax=cax, **kw)\n",
    "    \n",
    "    if not B_true==None:\n",
    "        mymax = max(B_true.max(), B.max())\n",
    "        mymin = min(B_true.min(), B.min())\n",
    "        im2 = ax4.imshow(B_true.cpu(),vmin=mymin,vmax=mymax)\n",
    "        ax4.set_title(\"B true\")\n",
    "        ax5.imshow(B.cpu(),vmin=mymin,vmax=mymax)\n",
    "        ax5.set_title(\"B model\")\n",
    "    else:\n",
    "        ax5.imshow(B.cpu())\n",
    "#     cax,kw = mpl.colorbar.make_axes([ax4, ax5])\n",
    "#     fig.colorbar(im2, cax=cax, **kw)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reg on C\n",
    "- what does A look like?\n",
    "- what does held-out look like?\n",
    "- multiple steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the PythonPath so python can find necessary code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, datetime\n",
    "LF_CODE_PATH = os.path.expanduser('~/projects/LFAnalyze/code')\n",
    "FT_CODE_PATH = os.path.expanduser('~/projects/fishTrax/code/analysis/')\n",
    "FD_CODE_PATH = os.path.expanduser('~/projects/fish_despair_notebooks/src/')\n",
    "sys.path.insert(0,LF_CODE_PATH)\n",
    "sys.path.insert(0,FT_CODE_PATH)\n",
    "sys.path.insert(0,FD_CODE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import useful python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pims\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import skimage.io\n",
    "import visualization_utils as vizutil\n",
    "import seaborn as sns\n",
    "from skimage.filters import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "passivity_2p_imaging_utils provides a list of all the datasets and provides a helper class to make loading the data easy.\n",
    "\n",
    "The data_sets are split into three conditions... (the keynames are wierd for historical reasons)   \n",
    "'enp': control group - no shocks at all.  \n",
    "'c': experimental group - fish experience behavioral challenge (repeated shocks) while being imaged  \n",
    "'e': reexposed group - first fish experience free-swimming behavioral challenge... then are imaged during behavioral challenge\n",
    "  \n",
    "all_data is a dictionary keyed by the condition.  \n",
    "all_data[condition] contains a list of Passivity_2p_Fish objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import passivity_2p_imaging_utils as p2putils\n",
    "reload(p2putils)\n",
    "tmp_dir = '/tmp/'\n",
    "all_data = p2putils.get_all_datasets(tmp_dir=tmp_dir)\n",
    "\n",
    "print('controls, n =', len(all_data['enp']))\n",
    "print('experimental, n =', len(all_data['c']))\n",
    "print('reexposed, n =', len(all_data['e']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ideal example fish for modeling\n",
    "(see 2p_Fish_Figure3_Accumulation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('experimental', all_data['enp'][5].fishid)\n",
    "print('experimental', all_data['c'][6].fishid)\n",
    "print('reexposed', all_data['e'][2].fishid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passivity_2p_Fish Class\n",
    "\n",
    "The `Passivity_2p_Fish` class offers various members and methods for loading data associated with each fish.  \n",
    "  \n",
    "Lets demo these methods using an example fish:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = all_data['e'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time of all volumes, tail_movements, and shocks are stored in the following variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first shock started at t=', f.get_shock_start_time(), 's')#shock start times\n",
    "print('The first slice of the frame began being imaged at t=', f.frame_st[0,0], 's') #time at which each slice was imaged [#samples X #slices]\n",
    "print('The first tail movement started at=', f.tail_movement_start_times[0], 's') #tail movement times - movements separted into forward swims, turns and escapes.\n",
    "print('Num z-planes imaged:', f.num_zplanes)\n",
    "print('Volume-Rate:', 1/np.diff(f.frame_st[:,0]).mean()) #frame_st is #frames x #slices, we examine interval between imaging first slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo code for examining movie of tail (slowed by factor of 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_ndx = 10\n",
    "clip = f.get_tail_movie_clip(f.tail_movement_start_times[movement_ndx]-.1, \n",
    "                             f.tail_movement_end_times[movement_ndx]+.1, \n",
    "                             playback_speed_factor=.2)\n",
    "clip.ipython_display(width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo code to plot tail movement rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_window_starts = np.arange(-300,31*60,60) + f.get_shock_start_time() #1 minutes windows around start of shock\n",
    "rate_window_centers = rate_window_starts + 30\n",
    "rate_windows_adj = rate_window_starts - f.td_time[0] #corrected for the fact that get_movement_rate wants time relative to start of tail imaging.\n",
    "plt.plot(rate_window_centers - f.get_shock_start_time(), [f.get_movement_rate([x,x+60], bExcShockResp=False) for x in rate_windows_adj])\n",
    "plt.axvline(0,c='r')\n",
    "plt.ylabel('Movement Rate (Hz)')\n",
    "plt.xlabel('Time Relative to First Shock (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_roi_table` method returns a dataframe of all the rois for teh fish.  Each row of this table represents an ROI and specified the place the ROI is in, the pixels that are included in the ROI, the centroid of the ROI, and which brain regions the ROI is in.  \n",
    "\n",
    "Note, this data is older and was processed by simply segmenting the anatomical images.  Thus the data is does not look as clean as data that is cleaned up and processed using CNMF, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = f.get_roi_table() #this can be slow to run the first time as data is loaded from files\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'get_signals_raw' returns a matrix containing the raw fluorescent signal associated with each ROI.  Each row of this matrix is associated with the corresponding row of the ROI table.  \n",
    "\n",
    "Note, I only use the second half of the signal matrix, because the agarose had not fully hardened during the first of imaging which cause the fish to drift in z slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = f.get_signals_raw(z=None)\n",
    "#M = hbutils.df_over_f(M)\n",
    "print('Num ROIs:', df.shape[0])\n",
    "print('Shape of signal matrix', M.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also various methods for grabbing the raw imaging data:  \n",
    "get_tif_as_vol  \n",
    "get_tif_rasl\n",
    "\n",
    "We can use this to visualize a few ROIs in particular plane/slice and brain region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=10\n",
    "\n",
    "#Create a background image by averaging 200 frames and adjusting the gamma.\n",
    "back_img = np.power(f.get_tif_rasl_as_vol(z,range(1,200)).mean(axis=2),.4)\n",
    "\n",
    "#Select rois in raphe in this slices, and get their coordinates.\n",
    "coords = df[(df.z==z)].coords\n",
    "\n",
    "#Overlay the ROIs on the background image and display:\n",
    "img = vizutil.overlay_coords(back_img, coords, [0,0,1], alpha=.5)\n",
    "plt.figure(figsize=[20,20])\n",
    "plt.imshow(img,interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poly_area(x,y):\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def get_roi_area(poly_coords):\n",
    "    \"\"\"Get area of an rois.\n",
    "    Args:\n",
    "    poly_coords: is numpy matrix #vertices x 2.  NAN rows mean coords separates multiple polygons.\n",
    "    \"\"\"\n",
    "    #split poly_coords into list of polys based on nan rows\n",
    "    poly_list = np.split(poly_coords, np.where(np.isnan(poly_coords).any(axis=1))[0])\n",
    "    \n",
    "    #clean up by removing nan rows\n",
    "    poly_list = [poly[~np.isnan(poly).any(axis=1)] for poly in poly_list] #remove nan\n",
    "    \n",
    "    areas = [get_poly_area(poly[:,0],poly[:,1]) for poly in poly_list]\n",
    "    return sum(areas)\n",
    "\n",
    "def get_cnmf_rois_in_region(self, z, region_name):\n",
    "    #return bool array of length num rois in plane. true if in region.\n",
    "    centroids = self.get_roi_centroids(z)\n",
    "    region_polys = self.region_polys\n",
    "    poly = region_polys[region_name]['polys'][z]\n",
    "    if len(poly) < 3:\n",
    "        return np.zeros(centroids.shape[0],dtype=bool)\n",
    "    else:\n",
    "        return mpl.path.Path(poly).contains_points(centroids)\n",
    "    \n",
    "def get_cnmf_roi_table_and_signals(self, z=None, region=None):\n",
    "    #z indexed from 0\n",
    "    assert(z==None or region==None)\n",
    "    if not hasattr(self, '_df_cnmf_all_rois'):\n",
    "        region_polys = self.region_polys\n",
    "        self._df_cnmf_all_rois = pd.DataFrame()\n",
    "        self._cnmf_signals_all = []\n",
    "\n",
    "        for nz in xrange(self.num_zplanes):\n",
    "            #Load CNMF results for each plane\n",
    "            fn_cnmf = self.data_prefix+'_plane%d_cnmf_final.npz'%(nz+1)\n",
    "            d = np.load(fn_cnmf)\n",
    "            contours = d['contours']\n",
    "            centroids = np.array([roi['CoM'] for roi in contours])\n",
    "            #Not some ROIs are not contigous... are split in two or more parts.\n",
    "            #In this case there is an NaN row in the poly list to separate sections\n",
    "            #the poly area funciton handles this.\n",
    "\n",
    "            #Add to dataframe\n",
    "            df_rois = pd.DataFrame()\n",
    "            df_rois['fishid'] = [self.fishid]*len(contours)\n",
    "            df_rois['z'] = np.ones(len(contours), dtype=int)*nz\n",
    "            df_rois['zndx'] = np.arange(len(contours), dtype=int)\n",
    "            df_rois['poly'] = [contour['coordinates'] for contour in contours]\n",
    "            df_rois['centroid_x'] = centroids[:,0]\n",
    "            df_rois['centroid_y'] = centroids[:,1]\n",
    "            df_rois['area'] = [get_roi_area(contour['coordinates']) for contour in contours]\n",
    "            for region_name in region_polys:\n",
    "                region_poly = region_polys[region_name]['polys'][nz]\n",
    "                if len(region_poly) < 3:\n",
    "                    region_bndx = np.zeros(centroids.shape[0],dtype=bool)\n",
    "                else:\n",
    "                    region_bndx = mpl.path.Path(region_poly).contains_points(centroids)\n",
    "                df_rois['in_'+region_name] = region_bndx\n",
    "            self._df_cnmf_all_rois = self._df_cnmf_all_rois.append(df_rois, ignore_index=True)\n",
    "\n",
    "            self._cnmf_signals_all.append(d['C'])\n",
    "\n",
    "\n",
    "        #Convert boolean region columsn to a column containing region name\n",
    "        reg_columns = [item.startswith('in_') for item in self._df_cnmf_all_rois.columns]\n",
    "        temp = np.array(self._df_cnmf_all_rois.ix[:,reg_columns])\n",
    "        reg_ndx = np.array([np.where(row)[0][0] if len(np.where(row)[0])>0 \n",
    "                                                else np.nan for row in temp])\n",
    "        reg_names = np.array([name[3:] for name in self._df_cnmf_all_rois.columns[reg_columns]])\n",
    "        reg_names = np.hstack([reg_names,['other']])\n",
    "        reg_ndx[np.isnan(reg_ndx)] = len(reg_names)-1\n",
    "        self._df_cnmf_all_rois['region'] = reg_names[reg_ndx.astype(int)]\n",
    "\n",
    "        #Split region in a hemisphere and bilateral region name\n",
    "        self._df_cnmf_all_rois['hemisphere'] = [reg[:1] if (reg.startswith('l_') or reg.startswith('r_')) \n",
    "                                       else 'w' for reg in self._df_cnmf_all_rois['region']]\n",
    "        self._df_cnmf_all_rois['region_bilat'] = [reg[2:] if (reg.startswith('l_') or reg.startswith('r_')) \n",
    "                                               else reg for reg in self._df_cnmf_all_rois['region']]\n",
    "\n",
    "        #stack signals\n",
    "        self._cnmf_signals_all = np.vstack(self._cnmf_signals_all)\n",
    "\n",
    "    if z is not None:\n",
    "        bndx = self._df_cnmf_all_rois['z']==z\n",
    "    elif region is not None:\n",
    "        bndx = self._df_cnmf_all_rois['in_'+region]==True\n",
    "    else:\n",
    "        bndx = np.ones(self._df_cnmf_all_rois.shape[0], dtype=bool)\n",
    "\n",
    "    return self._df_cnmf_all_rois[bndx].copy(), self._cnmf_signals_all[bndx,:].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=9\n",
    "df, sig = f.get_cnmf_roi_table_and_signals()\n",
    "\n",
    "#Create a background image by averaging 200 frames and adjusting the gamma.\n",
    "back_img = np.power(f.get_tif_rasl_as_vol(z,range(1,200)).mean(axis=2),.4)\n",
    "\n",
    "#Select rois in raphe in this slices, and get their coordinates.\n",
    "poly_coords = df[(df.z==z)].poly\n",
    "poly_coords = [np.round(poly[~np.isnan(poly).any(axis=1)])[:,(1,0)].astype(int) for poly in poly_coords]\n",
    "\n",
    "#Overlay the ROIs on the background image and display:\n",
    "img = vizutil.overlay_coords(back_img, poly_coords, [0,0,1], alpha=.5)\n",
    "plt.figure(figsize=[20,20])\n",
    "plt.imshow(img,interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, sig = f.get_cnmf_roi_table_and_signals()\n",
    "bndx = (df.in_r_MHb) & (df.z==4)\n",
    "plt.plot(sig[bndx,:].mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a,0)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    rm = ret[n - 1:] / n\n",
    "    pad_start = np.full((n-1,rm.shape[1]), rm[0])\n",
    "    return np.vstack([pad_start, rm])\n",
    "\n",
    "def ewma(data,span):\n",
    "    df = DataFrame(data)\n",
    "    return df.ewm(span).mean().values\n",
    "\n",
    "def df_f(x,ma_window=6,span=6):\n",
    "    u = moving_average(x,ma_window)\n",
    "    return ewma((x - u)/u, span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.array(bndx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = T.from_numpy(sig.T.astype(np.float32)).cuda()\n",
    "# M_T = T.from_numpy(M[bndx].T)\n",
    "\n",
    "# dff_norm = (dff - dff.mean(0))/(dff.std(0)+1e-8)\n",
    "# M_norm = (M_T - M_T.mean(0))/(M_T.std(0)+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = np.float32\n",
    "x_fish = F.normalize(dff,0)\n",
    "time_fish = T.from_numpy(f.frame_st.mean(1).astype(dtype)).cuda()\n",
    "if dtype==np.float16:\n",
    "    u_fish = T.cuda.HalfTensor(time_fish.shape).zero_()\n",
    "    p_fish = T.cuda.HalfTensor(time_fish.shape).zero_()\n",
    "else:\n",
    "    u_fish = T.cuda.FloatTensor(time_fish.shape).zero_()\n",
    "    p_fish = T.cuda.FloatTensor(time_fish.shape).zero_()\n",
    "u_fish[numpy.searchsorted(f.frame_et[:,-1], f.shock_st,side=\"left\")] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future_steps = 1\n",
    "batch_size = 32\n",
    "\n",
    "# data = FishSeqData(u_fish,p_fish,x_fish,n_future_steps)\n",
    "# model = DynamicsSeq(data.nfeatures,n_future_steps,T.float16)\n",
    "# train_fp16(model,data,15,(1e-4,1e-5),(1e-3,1e-4),lr=1e-3,verbose=False, loss_scale=512)\n",
    "\n",
    "ntrain = int(np.floor(len(x_fish)*0.8))\n",
    "data = FishSeqData(u_fish[:ntrain],p_fish[:ntrain],x_fish[:ntrain],n_future_steps)\n",
    "train_data = (u_fish[:ntrain],p_fish[:ntrain],x_fish[:ntrain])\n",
    "test_data = (u_fish[ntrain:],p_fish[ntrain:],x_fish[ntrain:])\n",
    "model = DynamicsSeq(data.nfeatures,n_future_steps)\n",
    "\n",
    "# data = FishSeqData(u_fish,p_fish,x_fish,n_future_steps)\n",
    "# model = DynamicsSeq(data.nfeatures,n_future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loss if A=B=C=zeros: {:3E}\".format(T.sum(T.abs(x_fish[1:]-x_fish[:-1]))/(x_fish.shape[0]-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,data,3,(1e-10,0),(1e-10,0),(1e-10,0), lr=1e-1,verbose=False)\n",
    "# train(model,data,6,(1e-0,0),(1e-0,0),(1e-0,0), lr=1e-2,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,P,X = test_data\n",
    "# U,P,X = train_data\n",
    "X_pred = model.predict(U,P,X[0])\n",
    "dx_true = X[1:] - X[:-1]\n",
    "dx_pred = X_pred[1:] - X_pred[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = predict(model, test_data).squeeze()[:,0,:]\n",
    "# pred = predict(model, data).squeeze()\n",
    "\n",
    "ncol, nrow = (5,3)\n",
    "fig, ax = plt.subplots(nrow,ncol, figsize=(18,10))\n",
    "\n",
    "# ax.set_ylabel(\"dx/dt\")\n",
    "# ax.set_xlabel(\"Time\")\n",
    "# ax.set_title(\"Training data\")\n",
    "\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        n = np.random.randint(0,len(x_fish))\n",
    "#         ax[i,j].plot(dx_pred[:,n].cpu().numpy(),color='red', alpha=0.6,linewidth=1, label=\"Model\")\n",
    "        ax[i,j].plot(X_pred[:,n].cpu().numpy(),color='red', alpha=0.6,linewidth=1, label=\"Model\")\n",
    "#         ax[i,j].plot(dx_true[:,n].cpu().numpy(),color=\"gray\", linewidth=1, alpha=0.6,label=\"Truth\")\n",
    "        ax[i,j].set_title(\"Neuron {}\".format(n))\n",
    "\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_regions = [u'in_r_cerebellum', u'in_l_cerebellum', u'in_l_vthal',\n",
    "       u'in_l_tectum', u'in_l_raphe', u'in_r_hind', u'in_l_hind',\n",
    "       u'in_l_dthal', u'in_r_tectum', u'in_r_LHb', u'in_r_dthal',\n",
    "       u'in_r_raphe', u'in_r_tel',\n",
    "       u'in_l_MHb', u'in_l_tel', u'in_r_MHb', u'in_l_LHb', u'in_r_vthal']\n",
    "\n",
    "regions = df.columns[np.where([c in ortho_regions for c in df.columns])]\n",
    "df = df.assign(region_id=pd.Series(np.full(len(df),-1).astype(np.int32)).values)\n",
    "for i, region in enumerate(regions):\n",
    "    idx = np.where(df[region])[0]\n",
    "    df.loc[idx, \"region_id\"] = i\n",
    "df[regions].sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros([4,4])\n",
    "x[[0,2],[0,2]] = 2\n",
    "df_test = DataFrame({\"region_id\":[0,1,0,1]})\n",
    "plot_matrix_by_region(x,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.from_numpy(x.astype(np.float32)).norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(row,regions):\n",
    "    return np.where(row[regions])[0][0]\n",
    "\n",
    "def plot_matrix_by_region(W, df):\n",
    "    neuron_map = np.argsort(df.where(df[\"region_id\"]>=0)[\"region_id\"]) # no region is -1\n",
    "    nHasRegion = np.sum(neuron_map!=-1) + 1 # last -1 index will be removed\n",
    "    newW = np.zeros(nHasRegion,nHasRegion)\n",
    "    new_order = neuron_map[np.arange(W.shape[0])]\n",
    "    new_order = new_order[new_order>=0]\n",
    "    newW = W[new_order]\n",
    "    newW = newW[:,new_order]\n",
    "    plt.imshow(newW,vmin=-1e-3,vmax=1e-3)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix_by_region(model.A.cpu().detach().numpy(),df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustergrid = sb.clustermap(model.A.cpu().detach().numpy(), method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustergrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"cnmf_f01555\",cnmf=sig.T, raw=M.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.A.()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.A.cpu().detach().numpy(),vmin=-0.005,vmax=0.005)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.eq(model.A,T.cuda.FloatTensor(model.A.shape).zero_()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.B[:100,:100].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.eq(model.B,T.cuda.FloatTensor(model.A.shape).zero_()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sort A by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.eq(model.A,T.cuda.FloatTensor(model.A.shape).zero_())[:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fish[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fish[0,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(u_fish[None,None,0],p_fish[None,None,0],x_fish[None,[0]],1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fish[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.B[:100,:100].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.save(model.state_dict(),\"fish_cnmf_decent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Half precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntime = 2826\n",
    "nstim = 30\n",
    "nfeatures = 15888\n",
    "\n",
    "u_train = T.cuda.HalfTensor(ntime).uniform_()\n",
    "p_train = T.cuda.HalfTensor(ntime).uniform_()\n",
    "time_train = T.arange(ntime).half().cuda()\n",
    "x_train = T.cuda.HalfTensor(ntime,nfeatures).uniform_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ntime*3+ntime*nfeatures)*16/1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future_steps = 1\n",
    "batch_size = 1\n",
    "\n",
    "data = FishSeqData(u_train,p_train,x_train,n_future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicsSeq(data.nfeatures,n_future_steps,T.float16,1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp16(model,data,15,(1e-4,1e-5),(1e-3,0),lr=1e-3,verbose=False, loss_scale=512)\n",
    "time_test, u_test, p_test, x_test = test_data\n",
    "model_v_truth(model, u_test, p_test, x_test, A_true.data, B_true.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntime = 2826\n",
    "nstim = 30\n",
    "nfeatures = 15888\n",
    "\n",
    "u_train = T.cuda.FloatTensor(ntime).uniform_()\n",
    "p_train = T.cuda.FloatTensor(ntime).uniform_()\n",
    "time_train = T.arange(ntime).cuda()\n",
    "x_train = T.cuda.FloatTensor(ntime,nfeatures).uniform_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future_steps = 1\n",
    "batch_size = 1\n",
    "\n",
    "data = FishSeqData(u_train,p_train,x_train,n_future_steps)\n",
    "model = DynamicsSeq(data.nfeatures,n_future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,data,1,(1e-4,1e-5),(1e-3,1e-4),lr=0.1,verbose=False)\n",
    "# time_test, u_test, p_test, x_test = test_data\n",
    "# model_v_truth(model, u_test, p_test, x_test, A_true.data, B_true.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntime = 3000\n",
    "nstim = 30\n",
    "nfeatures = 10\n",
    "A_true, B_true, C_true, train_data, test_data = generate_data(ntime, nstim,nfeatures,burn_in=200, e=0.1,k=1,t0=1e10)\n",
    "\n",
    "time_train, u_train, p_train, x_train = train_data\n",
    "time_test, u_test, p_test, x_test = test_data\n",
    "\n",
    "scale = 8\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].plot(time_train,p_train.cpu().numpy())\n",
    "ax[0].set_ylabel(\"Passivity\")\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "\n",
    "ax[1].plot(x_train[:,2].cpu().numpy())\n",
    "ax[1].set_title(\"Ground Truth\")\n",
    "ax[1].set_ylabel(\"df/f\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "fig.tight_layout()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future_steps = 1\n",
    "batch_size = 10\n",
    "\n",
    "data = FishSeqData(u_train,p_train,x_train,n_future_steps)\n",
    "model = DynamicsSeq(data.nfeatures,n_future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,data,8,(1e-4,0),(1e-3,1e-4),lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v_truth(model, u_test, p_test, x_test, A_true.data, B_true.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future_steps = 1\n",
    "batch_size = 1\n",
    "\n",
    "scale = 1\n",
    "time_train, u_train, p_train, x_train = time_train, u_train.half()*scale, p_train.half()*scale, x_train.half()*scale\n",
    "time_test, u_test, p_test, x_test = time_test, u_test.half()*scale, p_test.half()*scale, x_test.half()*scale\n",
    "data = FishSeqData(u_train,p_train,x_train,n_future_steps)\n",
    "model = DynamicsSeq(data.nfeatures,n_future_steps,T.float16,scale)\n",
    "train_fp16(model,data,15,(1e-4,1e-5),(1e-3,0),lr=1e-3,verbose=True, loss_scale=512)\n",
    "\n",
    "model_v_truth(model, u_test, p_test, x_test, A_true.data, B_true.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p27)",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
